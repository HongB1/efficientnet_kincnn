{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "Number of positive samples in training data: 1605 (50.03% of total)\n",
      "Number of positive samples in validation data: 395 (49.87% of total)\n"
     ]
    }
   ],
   "source": [
    "from prepare_dataset import prepare_dataset\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "data = prepare_dataset()\n",
    "train_set = data[0]\n",
    "valid_set = data[1]\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_set, batch_size=64, pin_memory=True, shuffle=True)\n",
    "valid_loader = data_utils.DataLoader(valid_set, batch_size=64,)\n",
    "\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "dataloaders = {'train':train_loader,'valid':valid_loader}\n",
    "dataset_sizes = {x: len(dataloaders[x]) for x in ['train', 'valid']}\n",
    "dataset = ConcatDataset([train_set, valid_set])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet-phospho-B-15\n",
      "3200\n",
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=32, weight of size [32, 1, 3, 3], expected input[64, 8, 134, 17] to have 32 channels, but got 8 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 38\u001B[0m\n\u001B[1;32m     36\u001B[0m inputs \u001B[38;5;241m=\u001B[39m Variable(inputs\u001B[38;5;241m.\u001B[39mto(device, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat), requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     37\u001B[0m labels \u001B[38;5;241m=\u001B[39m Variable(labels\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[0;32m---> 38\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/efficientnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1500\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1502\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/efficientnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1506\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1507\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1509\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1510\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1512\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/Users/hb/Dropbox/github/efficientnet_kincnn/model.py:292\u001B[0m, in \u001B[0;36mEfficientNet.forward\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\u001B[39;00m\n\u001B[1;32m    289\u001B[0m \u001B[38;5;66;03m# bs = inputs.size(0)\u001B[39;00m\n\u001B[1;32m    290\u001B[0m \u001B[38;5;66;03m# print(bs)\u001B[39;00m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;66;03m# Convolution layers\u001B[39;00m\n\u001B[0;32m--> 292\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;66;03m# Pooling and final linear layer\u001B[39;00m\n\u001B[1;32m    295\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_avg_pooling(x)\n",
      "File \u001B[0;32m/Users/hb/Dropbox/github/efficientnet_kincnn/model.py:278\u001B[0m, in \u001B[0;36mEfficientNet.extract_features\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m drop_connect_rate:\n\u001B[1;32m    277\u001B[0m         drop_connect_rate \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(idx) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocks)\n\u001B[0;32m--> 278\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdrop_connect_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdrop_connect_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;66;03m# Head\u001B[39;00m\n\u001B[1;32m    281\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conv_head(x)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/efficientnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1500\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1502\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/efficientnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1506\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1507\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1509\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1510\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1512\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/Users/hb/Dropbox/github/efficientnet_kincnn/model.py:91\u001B[0m, in \u001B[0;36mMBConvBlock.forward\u001B[0;34m(self, inputs, drop_connect_rate)\u001B[0m\n\u001B[1;32m     88\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bn0(x)\n\u001B[1;32m     89\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mish(x)\n\u001B[0;32m---> 91\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_depthwise_conv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bn1(x)\n\u001B[1;32m     93\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mish(x)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/efficientnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1500\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1502\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/efficientnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1506\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1507\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1509\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1510\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1512\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/Users/hb/Dropbox/github/efficientnet_kincnn/utils.py:275\u001B[0m, in \u001B[0;36mConv2dStaticSamePadding.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m    274\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstatic_padding(x)\n\u001B[0;32m--> 275\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Given groups=32, weight of size [32, 1, 3, 3], expected input[64, 8, 134, 17] to have 32 channels, but got 8 channels instead"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from model import EfficientNet\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "for fold, (train_idx, valid_idx) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "    model = EfficientNet.from_name('efficientnet-phospho-B-15')\n",
    "    model = model.to(device)\n",
    "   # Define data loaders for training and testing data in this fold\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    print(len(train_idx))\n",
    "    print(len(valid_idx))\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                dataset,\n",
    "                batch_size=64, sampler=train_subsampler)\n",
    "    validloader = torch.utils.data.DataLoader(\n",
    "                dataset,\n",
    "                batch_size=64, sampler=test_subsampler)\n",
    "\n",
    "    for epoch in tqdm(range(20), position=0, leave=True):\n",
    "        # print('-' * 60)\n",
    "        # print('Epoch {}/{}'.format(epoch+1, 20))\n",
    "\n",
    "        train_corrects = 0.0\n",
    "        train_loss = 0.0\n",
    "        train_precision, train_recall, train_f1 = 0.0, 0.0, 0.0\n",
    "\n",
    "        for _, (inputs, labels) in enumerate(tqdm(trainloader, position=1, leave=True)):\n",
    "                model.train(True)\n",
    "                inputs = Variable(inputs.to(device, dtype=torch.float), requires_grad=True)\n",
    "                labels = Variable(labels.to(device))\n",
    "                pred = model(inputs) # forward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import torch\n",
    "from torch import nn\n",
    "from utils import *\n",
    "from torch.nn import functional as F\n",
    "\n",
    "GlobalParams = collections.namedtuple(\n",
    "    \"GlobalParams\",\n",
    "    [\n",
    "        \"width_coefficient\",\n",
    "        \"depth_coefficient\",\n",
    "        \"image_size\",\n",
    "        \"dropout_rate\",\n",
    "        \"num_classes\",\n",
    "        \"batch_norm_momentum\",\n",
    "        \"batch_norm_epsilon\",\n",
    "        \"drop_connect_rate\",\n",
    "        \"depth_divisor\",\n",
    "        \"min_depth\",\n",
    "        \"include_top\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Parameters for an individual model block\n",
    "BlockArgs = collections.namedtuple(\n",
    "    \"BlockArgs\",\n",
    "    [\n",
    "        \"num_repeat\",\n",
    "        \"kernel_size\",\n",
    "        \"stride\",\n",
    "        \"expand_ratio\",\n",
    "        \"input_filters\",\n",
    "        \"output_filters\",\n",
    "        \"se_ratio\",\n",
    "        \"id_skip\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Set GlobalParams and BlockArgs's defaults\n",
    "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n",
    "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "def efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n",
    "                 drop_connect_rate=0.2, image_size=None, num_classes=1):\n",
    "    \"\"\" Creates a efficientnet model. \"\"\"\n",
    "\n",
    "    blocks_args = [\n",
    "        'r1_kh8_kw2_sh2_sw2_e2_i8_o16_se0.25',\n",
    "        'r1_kh8_kw3_sh2_sw1_e2_i16_o32_se0.25',\n",
    "        'r1_kh6_kw3_sh2_sw2_e2_i32_o64_se0.25',\n",
    "        # 'r3_k3_s22_e6_i40_o80_se0.25',\n",
    "        # 'r3_k5_s11_e6_i80_o112_se0.25',\n",
    "        # 'r4_k5_s22_e6_i112_o192_se0.25',\n",
    "        # 'r1_k3_s11_e6_i192_o320_se0.25',\n",
    "        # 'r1_k3_s11_e6_i24_o48_se0.25',\n",
    "    ]\n",
    "    blocks_args = BlockDecoder.decode(blocks_args)\n",
    "\n",
    "    global_params = GlobalParams(\n",
    "        batch_norm_momentum=0.99,\n",
    "        batch_norm_epsilon=1e-3,\n",
    "        dropout_rate=dropout_rate,\n",
    "        drop_connect_rate=drop_connect_rate,\n",
    "        # data_format='channels_last',  # removed, this is always true in PyTorch\n",
    "        num_classes=num_classes,\n",
    "        width_coefficient=width_coefficient,\n",
    "        depth_coefficient=depth_coefficient,\n",
    "        depth_divisor=8,\n",
    "        min_depth=None,\n",
    "        image_size=image_size,\n",
    "    )\n",
    "\n",
    "    return blocks_args, global_params\n",
    "\n",
    "blocks_args, global_params = efficientnet(width_coefficient=1.0, depth_coefficient=1.0, dropout_rate=0.7, image_size=[263, 15])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalParams(width_coefficient=1.0, depth_coefficient=1.0, image_size=[263, 15], dropout_rate=0.7, num_classes=1, batch_norm_momentum=0.99, batch_norm_epsilon=0.001, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, include_top=None)\n",
      "\n",
      "MBConvblock 0:  BlockArgs(num_repeat=1, kernel_size=(8, 2), stride=(2, 2), expand_ratio=2, input_filters=8, output_filters=16, se_ratio=0.25, id_skip=True)\n",
      "\n",
      "MBConvblock 1:  BlockArgs(num_repeat=1, kernel_size=(8, 3), stride=(2, 1), expand_ratio=2, input_filters=16, output_filters=32, se_ratio=0.25, id_skip=True)\n",
      "\n",
      "MBConvblock 2:  BlockArgs(num_repeat=1, kernel_size=(6, 3), stride=(2, 2), expand_ratio=2, input_filters=32, output_filters=64, se_ratio=0.25, id_skip=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(global_params)\n",
    "print()\n",
    "for idx, block_args in enumerate(blocks_args):\n",
    "    print(f'MBConvblock {idx}: ', block_args)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [],
   "source": [
    "def calculate_output_image_size_(input_image_size, stride):\n",
    "    \"\"\"Calculates the output image size when using Conv2dSamePadding with a stride.\n",
    "       Necessary for static padding. Thanks to mannatsingh for pointing this out.\n",
    "\n",
    "    Args:\n",
    "        input_image_size (int, tuple or list): Size of input image.\n",
    "        stride (int, tuple or list): Conv2d operation's stride.\n",
    "\n",
    "    Returns:\n",
    "        output_image_size: A list [H,W].\n",
    "    \"\"\"\n",
    "    if input_image_size is None:\n",
    "        return None\n",
    "    image_height, image_width = get_width_and_height_from_size(input_image_size)\n",
    "    sh, sw = stride if type(stride) == (list) or (tuple) else [stride, stride]\n",
    "    # stride = stride if isinstance(stride, int) else stride[0]\n",
    "    image_height = int(math.ceil(image_height / sh))\n",
    "    image_width = int(math.ceil(image_width / sw))\n",
    "    return [image_height, image_width]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [],
   "source": [
    "bn_mom = 1 - global_params.batch_norm_momentum\n",
    "bn_eps = global_params.batch_norm_epsilon\n",
    "image_size = global_params.image_size\n",
    "Conv2d = partial(Conv2dStaticSamePadding, image_size=image_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "data": {
      "text/plain": "[132, 15]"
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem\n",
    "image_size = [263, 15]\n",
    "in_channels = 1\n",
    "out_channels = round_filters(8, global_params)\n",
    "_conv_stem = Conv2d(in_channels, out_channels, kernel_size=(15, 1), stride=(2, 1), bias=False, image_size=image_size)\n",
    "_bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "image_size = calculate_output_image_size_(image_size, stride=(2, 1))\n",
    "image_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "_swish = MemoryEfficientSwish()\n",
    "x0 = torch.randn((1, 1, 263, 15))\n",
    "x1 = _conv_stem(x0)\n",
    "x2 = _bn0(x1)\n",
    "x3 = _swish(x2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 8, 132, 15])"
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBConvblock 0:  BlockArgs(num_repeat=1, kernel_size=(8, 2), stride=(2, 2), expand_ratio=2, input_filters=8, output_filters=16, se_ratio=0.25, id_skip=True)\n",
      "\n",
      "MBConvblock 1:  BlockArgs(num_repeat=1, kernel_size=(8, 3), stride=(2, 1), expand_ratio=2, input_filters=16, output_filters=32, se_ratio=0.25, id_skip=True)\n",
      "\n",
      "MBConvblock 2:  BlockArgs(num_repeat=1, kernel_size=(6, 3), stride=(2, 2), expand_ratio=2, input_filters=32, output_filters=64, se_ratio=0.25, id_skip=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, block_args in enumerate(blocks_args):\n",
    "    print(f'MBConvblock {idx}: ', block_args)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [
    "# 263, 15 -> 132, 8 -> 64, 15 -> 32, 15 -> 17, 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [],
   "source": [
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Mobile Inverted Residual Bottleneck Block\n",
    "    Args:\n",
    "        block_args (namedtuple): BlockArgs, see above\n",
    "        global_params (namedtuple): GlobalParam, see above\n",
    "    Attributes:\n",
    "        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_args, global_params, image_size=None):\n",
    "        super().__init__()\n",
    "        self._block_args = block_args\n",
    "        self._bn_mom = 1 - global_params.batch_norm_momentum\n",
    "        self._bn_eps = global_params.batch_norm_epsilon\n",
    "        self.has_se = (self._block_args.se_ratio is not None) and (\n",
    "            0 < self._block_args.se_ratio <= 1\n",
    "        )\n",
    "        self.id_skip = block_args.id_skip  # skip connection and drop connect\n",
    "\n",
    "        # Expansion phase (Inverted Bottleneck)\n",
    "        inp = self._block_args.input_filters  # number of input channels\n",
    "        oup = (\n",
    "            self._block_args.input_filters * self._block_args.expand_ratio\n",
    "        )  # number of output channels\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            Conv2d = partial(Conv2dStaticSamePadding, image_size=image_size)\n",
    "            self._expand_conv = Conv2d(\n",
    "                in_channels=inp, out_channels=oup, kernel_size=1, bias=False\n",
    "            )\n",
    "            self._bn0 = nn.BatchNorm2d(\n",
    "                num_features=oup, momentum=self._bn_mom, eps=self._bn_eps\n",
    "            )\n",
    "            # image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        k = self._block_args.kernel_size\n",
    "        s = self._block_args.stride\n",
    "        Conv2d = partial(Conv2dStaticSamePadding, image_size=image_size)\n",
    "        self._depthwise_conv = Conv2d(\n",
    "            in_channels=oup,\n",
    "            out_channels=oup,\n",
    "            groups=oup,  # groups makes it depthwise\n",
    "            kernel_size=k,\n",
    "            stride=s,\n",
    "            bias=False,\n",
    "        )\n",
    "        self._bn1 = nn.BatchNorm2d(\n",
    "            num_features=oup, momentum=self._bn_mom, eps=self._bn_eps\n",
    "        )\n",
    "        image_size = calculate_output_image_size_(image_size, s)\n",
    "\n",
    "        # Squeeze and Excitation layer, if desired\n",
    "        if self.has_se:\n",
    "            Conv2d = get_same_padding_conv2d(image_size=(1, 1))\n",
    "            num_squeezed_channels = max(\n",
    "                1, int(self._block_args.input_filters * self._block_args.se_ratio)\n",
    "            )\n",
    "            self._se_reduce = Conv2d(\n",
    "                in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1\n",
    "            )\n",
    "            self._se_expand = Conv2d(\n",
    "                in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1\n",
    "            )\n",
    "\n",
    "        # Output phase (Pointwise convolution phase)\n",
    "        final_oup = self._block_args.output_filters\n",
    "        Conv2d = partial(Conv2dStaticSamePadding, image_size=image_size)\n",
    "        self._project_conv = Conv2d(\n",
    "            in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False\n",
    "        )\n",
    "        self._bn2 = nn.BatchNorm2d(\n",
    "            num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps\n",
    "        )\n",
    "        self._swish = MemoryEfficientSwish()\n",
    "\n",
    "    def forward(self, inputs, drop_connect_rate=None):\n",
    "        \"\"\"\n",
    "        :param inputs: input tensor\n",
    "        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
    "        :return: output of block\n",
    "        \"\"\"\n",
    "\n",
    "        # Expansion and Depthwise Convolution\n",
    "        x = inputs\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            x = self._expand_conv(inputs)\n",
    "            x = self._bn0(x)\n",
    "            x = self._swish(x)\n",
    "\n",
    "        x = self._depthwise_conv(x)\n",
    "        x = self._bn1(x)\n",
    "        x = self._swish(x)\n",
    "\n",
    "        # Squeeze and Excitation\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "            x_squeezed = self._se_reduce(x_squeezed)\n",
    "            x_squeezed = self._swish(x_squeezed)\n",
    "            x_squeezed = self._se_expand(x_squeezed)\n",
    "            x = torch.sigmoid(x_squeezed) * x\n",
    "\n",
    "        # Pointwise Convolution\n",
    "        x = self._project_conv(x)\n",
    "        x = self._bn2(x)\n",
    "\n",
    "        # Skip connection and drop connect\n",
    "        input_filters, output_filters = (\n",
    "            self._block_args.input_filters,\n",
    "            self._block_args.output_filters,\n",
    "        )\n",
    "        if (\n",
    "            self.id_skip\n",
    "            and self._block_args.stride == 1\n",
    "            and input_filters == output_filters\n",
    "        ):\n",
    "            # The combination of skip connection and drop connect brings about stochastic depth.\n",
    "            if drop_connect_rate:\n",
    "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
    "            x = x + inputs  # skip connection\n",
    "        return x\n",
    "\n",
    "    def set_swish(self, memory_efficient=True):\n",
    "        \"\"\"Sets swish function as memory efficient (for training) or standard (for export).\n",
    "\n",
    "        Args:\n",
    "            memory_efficient (bool): Whether to use memory-efficient version of swish.\n",
    "        \"\"\"\n",
    "        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "_bn_mom = 1 - global_params.batch_norm_momentum\n",
    "_bn_eps = global_params.batch_norm_epsilon\n",
    "\n",
    "has_se = (block_args.se_ratio is not None) and (0 < block_args.se_ratio <= 1)\n",
    "id_skip = block_args.id_skip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "# Expansion phase (Inverted Bottleneck)\n",
    "inp = block_args.input_filters  # number of input channels\n",
    "oup = (\n",
    "    block_args.input_filters * block_args.expand_ratio\n",
    ")  # number of output channels\n",
    "if block_args.expand_ratio != 1:\n",
    "    Conv2d = partial(Conv2dStaticSamePadding, image_size=image_size)\n",
    "    _expand_conv = Conv2d(\n",
    "        in_channels=inp, out_channels=oup, kernel_size=1, bias=False\n",
    "    )\n",
    "    _bn0 = nn.BatchNorm2d(\n",
    "        num_features=oup, momentum=_bn_mom, eps=_bn_eps\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "# Depthwise convolution phase\n",
    "k = block_args.kernel_size\n",
    "s = block_args.stride\n",
    "Conv2d = partial(Conv2dStaticSamePadding, image_size=image_size)\n",
    "_depthwise_conv = Conv2d(\n",
    "    in_channels=oup,\n",
    "    out_channels=oup,\n",
    "    groups=oup,  # groups makes it depthwise\n",
    "    kernel_size=k,\n",
    "    stride=s,\n",
    "    bias=False,\n",
    ")\n",
    "_bn1 = nn.BatchNorm2d(\n",
    "    num_features=oup, momentum=_bn_mom, eps=_bn_eps\n",
    ")\n",
    "image_size = calculate_output_image_size_(image_size, s)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "# Squeeze and Excitation layer, if desired\n",
    "if has_se:\n",
    "    Conv2d = get_same_padding_conv2d(image_size=(1, 1))\n",
    "    num_squeezed_channels = max(\n",
    "        1, int(block_args.input_filters * block_args.se_ratio)\n",
    "    )\n",
    "    _se_reduce = Conv2d(\n",
    "        in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1\n",
    "    )\n",
    "    _se_expand = Conv2d(\n",
    "        in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1\n",
    "    )\n",
    "\n",
    "# Output phase (Pointwise convolution phase)\n",
    "final_oup = block_args.output_filters\n",
    "Conv2d = partial(Conv2dStaticSamePadding, image_size=image_size)\n",
    "_project_conv = Conv2d(\n",
    "    in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False\n",
    ")\n",
    "_bn2 = nn.BatchNorm2d(\n",
    "    num_features=final_oup, momentum=_bn_mom, eps=_bn_eps\n",
    ")\n",
    "_swish = MemoryEfficientSwish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "data": {
      "text/plain": "BlockArgs(num_repeat=1, kernel_size=(8, 2), stride=(2, 2), expand_ratio=1, input_filters=8, output_filters=16, se_ratio=0.25, id_skip=True)"
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_args"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "inputs = x3\n",
    "inputs.shape\n",
    "x = inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 66, 7])\n",
      "torch.Size([1, 8, 66, 7])\n",
      "torch.Size([1, 8, 66, 7])\n"
     ]
    }
   ],
   "source": [
    "x = _depthwise_conv(x)\n",
    "print(x.shape)\n",
    "x = _bn1(x)\n",
    "print(x.shape)\n",
    "x = _swish(x)\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 1, 1])\n",
      "torch.Size([1, 2, 1, 1])\n",
      "torch.Size([1, 2, 1, 1])\n",
      "torch.Size([1, 8, 1, 1])\n",
      "torch.Size([1, 8, 66, 7])\n"
     ]
    }
   ],
   "source": [
    "if has_se:\n",
    "    x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "    print(x_squeezed.shape)\n",
    "    x_squeezed = _se_reduce(x_squeezed)\n",
    "    print(x_squeezed.shape)\n",
    "    x_squeezed = _swish(x_squeezed)\n",
    "    print(x_squeezed.shape)\n",
    "    x_squeezed = _se_expand(x_squeezed)\n",
    "    print(x_squeezed.shape)\n",
    "    x = torch.sigmoid(x_squeezed) * x\n",
    "    print(x.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 16, 66, 7])"
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "x = _project_conv(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 66, 7])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 66, 7])\n"
     ]
    }
   ],
   "source": [
    "# Pointwise Convolution\n",
    "# x = _project_conv\n",
    "\n",
    "x = _bn2(x)\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "data": {
      "text/plain": "(8, 16)"
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Skip connection and drop connect\n",
    "input_filters, output_filters = (\n",
    "    block_args.input_filters,\n",
    "    block_args.output_filters,\n",
    ")\n",
    "input_filters, output_filters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if (\n",
    "    id_skip\n",
    "    and block_args.stride == 1\n",
    "    and input_filters == output_filters\n",
    "):\n",
    "    # The combination of skip connection and drop connect brings about stochastic depth.\n",
    "    if drop_connect_rate:\n",
    "        x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
    "    x = x + inputs  # skip connection\n",
    "return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  [132, 15]\n",
      "After:  [66, 8]\n",
      "Before:  [66, 8]\n",
      "After:  [33, 8]\n",
      "Before:  [33, 8]\n",
      "After:  [17, 4]\n"
     ]
    }
   ],
   "source": [
    "# MBConvBlock\n",
    "\n",
    "blocks = nn.ModuleList([])\n",
    "for block_args in blocks_args:\n",
    "    # Update block input and output filters based on depth multiplier.\n",
    "    block_args = block_args._replace(\n",
    "        input_filters=round_filters(\n",
    "            block_args.input_filters, global_params\n",
    "        ),\n",
    "        # input_filters=8,\n",
    "        output_filters=round_filters(\n",
    "            block_args.output_filters, global_params\n",
    "        ),\n",
    "        num_repeat=round_repeats(block_args.num_repeat, global_params),\n",
    "    )\n",
    "\n",
    "    # The first block needs to take care of stride and filter size increase.\n",
    "    blocks.append(\n",
    "        MBConvBlock(block_args, global_params, image_size=image_size)\n",
    "    )\n",
    "    print('Before: ', image_size)\n",
    "    image_size = calculate_output_image_size_(image_size, block_args.stride)\n",
    "    print('After: ', image_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "[17, 4]"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "# Head\n",
    "in_channels = block_args.output_filters  # output of final block\n",
    "out_channels = round_filters(128, global_params)\n",
    "_conv_head = Conv2d(in_channels, out_channels, kernel_size=(2, 1), bias=False)\n",
    "_bn1 = nn.BatchNorm2d(\n",
    "    num_features=out_channels, momentum=bn_mom, eps=bn_eps\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "_dropout = nn.Dropout(global_params.dropout_rate)\n",
    "_fc = nn.Linear(out_channels, global_params.num_classes)\n",
    "\n",
    "# set activation to memory efficient swish by default\n",
    "_swish = MemoryEfficientSwish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 8, 132, 15])"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem\n",
    "x0 = torch.randn((1, 1, 263, 15))\n",
    "x1 = _conv_stem(x0)\n",
    "x2 = _bn0(x1)\n",
    "x3 = _swish(x2)\n",
    "x3.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "x = x3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 8, 132, 15])\n",
      "torch.Size([1, 16, 66, 8])\n",
      "torch.Size([1, 16, 66, 8])\n",
      "torch.Size([1, 32, 33, 8])\n",
      "torch.Size([1, 32, 33, 8])\n",
      "torch.Size([1, 64, 17, 4])\n"
     ]
    }
   ],
   "source": [
    "print(len(blocks))\n",
    "\n",
    "for idx, block in enumerate(blocks):\n",
    "    print(x.shape)\n",
    "    drop_connect_rate = global_params.drop_connect_rate\n",
    "    if drop_connect_rate:\n",
    "        drop_connect_rate *= float(idx) / len(blocks)\n",
    "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "    print(x.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 64, 17, 4])"
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [
    {
     "data": {
      "text/plain": "BlockArgs(num_repeat=1, kernel_size=(6, 3), stride=(2, 2), expand_ratio=2, input_filters=32, output_filters=64, se_ratio=0.25, id_skip=True)"
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_args"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [],
   "source": [
    "in_channels = block_args.output_filters  # output of final block\n",
    "out_channels = round_filters(128, global_params)\n",
    "# out_channels = round_filters(1280, self._global_params) <-- 원본\n",
    "# out_channels = 1280\n",
    "_conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "_bn1 = nn.BatchNorm2d(\n",
    "    num_features=out_channels, momentum=bn_mom, eps=bn_eps)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 128, 17, 4])"
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = _conv_head(x)\n",
    "x = _bn1(x)\n",
    "x = _swish(x)\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# x = _avg_pooling(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "outputs": [
    {
     "data": {
      "text/plain": "128"
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_channels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [],
   "source": [
    "_avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "_dropout = nn.Dropout(global_params.dropout_rate)\n",
    "_fc = nn.Linear(out_channels, global_params.num_classes)\n",
    "\n",
    "# set activation to memory efficient swish by default\n",
    "_swish = MemoryEfficientSwish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "outputs": [],
   "source": [
    "x_copy = x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.3333]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = _avg_pooling(x)\n",
    "y = y.flatten(start_dim=1)\n",
    "y = _dropout(y)\n",
    "y = _fc(y)\n",
    "y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 128])"
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y = y.view(1, -1)\n",
    "y.shape\n",
    "# print(x.shape)\n",
    "\n",
    "# x = x.flatten(start_dim=1)\n",
    "# x = self._dropout(x)\n",
    "\n",
    "# x = self._fc(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.1952, 0.2066, 0.1998, 0.2097, 0.2002, 0.2060, 0.2034, 0.2010, 0.1997,\n         0.2035, 0.2030, 0.1976, 0.1983, 0.1964, 0.2027, 0.2046, 0.2035, 0.2029,\n         0.2082, 0.2109, 0.2003, 0.2054, 0.2026, 0.2103, 0.1970, 0.2057, 0.1945,\n         0.2000, 0.1880, 0.2057, 0.1968, 0.2006, 0.2054, 0.2093, 0.1914, 0.2065,\n         0.2077, 0.2035, 0.2096, 0.1974, 0.2065, 0.1984, 0.2047, 0.2102, 0.2056,\n         0.2111, 0.2112, 0.2067, 0.2039, 0.2021, 0.2035, 0.2015, 0.1834, 0.1905,\n         0.2059, 0.2052, 0.2052, 0.1892, 0.2001, 0.1991, 0.2050, 0.2024, 0.2093,\n         0.2104, 0.2007, 0.2100, 0.1988, 0.1895, 0.1999, 0.2009, 0.2069, 0.2135,\n         0.1948, 0.2116, 0.1986, 0.1980, 0.1937, 0.1970, 0.2007, 0.2006, 0.1952,\n         0.2038, 0.2047, 0.2028, 0.1999, 0.1964, 0.2048, 0.2022, 0.2015, 0.2061,\n         0.2032, 0.2024, 0.1970, 0.1846, 0.1995, 0.1991, 0.2061, 0.2078, 0.1917,\n         0.2001, 0.2058, 0.1946, 0.2025, 0.1998, 0.2066, 0.2030, 0.1995, 0.2035,\n         0.1996, 0.2079, 0.1934, 0.2043, 0.2035, 0.2096, 0.2030, 0.2057, 0.2047,\n         0.2009, 0.2005, 0.2158, 0.2031, 0.1896, 0.2059, 0.2012, 0.2117, 0.2055,\n         0.2035, 0.2006]], grad_fn=<ViewBackward0>)"
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
