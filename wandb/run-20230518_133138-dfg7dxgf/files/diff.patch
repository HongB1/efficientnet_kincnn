diff --git a/code/C4_evaluation.ipynb b/code/C4_evaluation.ipynb
index a77475b..63ce1e9 100644
--- a/code/C4_evaluation.ipynb
+++ b/code/C4_evaluation.ipynb
@@ -14,47 +14,1566 @@
     "import kincnn\n",
     "import kincnn2\n",
     "import kincnn3\n",
-    "import kincnn4"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "metadata": {},
-   "outputs": [],
-   "source": [
+    "import kincnn4\n",
     "import sys\n",
-    "# sys.path.append('/home/hb/anaconda3/envs/pp_predict/lib/python3.9/site-packages')\n",
-    "# sys.path.append('/home/hb/python/neoantigen/from_ju/IEDB_data_filtering')\n",
-    "# sys.path.append('/home/hb/python/neoantigen/code/module')\n",
     "sys.path.append('/home/hb/python/phospho/code/module')\n",
     "import pickle\n",
     "import numpy as np\n",
     "import pandas as pd\n",
     "from tqdm.notebook import tqdm\n",
-    "# from benchmark_util import *\n",
     "import torch\n",
-    "from efficientnet_new import *\n",
     "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
     "import sklearn\n",
-    "import seaborn as sns\n",
-    "import matplotlib\n",
     "import matplotlib.pyplot as plt\n",
-    "import numpy as np\n",
-    "import statistics\n",
-    "import itertools"
+    "import numpy as np"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 4,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from utils import efficientnet\n",
+    "a, b = efficientnet()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "[BlockArgs(num_repeat=1, conv_kernel_size=(5, 1), pool_kernel_size=None, conv_stride=(1, 1), pool_stride=(3, 1), expand_ratio=1, input_filters=8, output_filters=6, se_ratio=0.25, id_skip=True)]"
+      ]
+     },
+     "execution_count": 5,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "a"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "checkpoint = torch.load(model_path_list[i], map_location='cpu')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 15,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "filedir = '/home/hb/python/efficientnet_kincnn/saved_model/0518/DeepPP_kincnn1_1251_bs1024_weight0'\n",
+    "model_path_list = [f'{filedir}/{file}' for file in os.listdir(filedir) if 'best' in file]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 16,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "{'model': EfficientNet(\n",
+       "   (_conv_stem): Conv2dStaticSamePadding(\n",
+       "     1, 8, kernel_size=(3, 1), stride=(1, 1), bias=False\n",
+       "     (static_padding): ZeroPad2d((0, 0, 1, 1))\n",
+       "   )\n",
+       "   (_bn0): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "   (_max_pooling): MaxPool2dStaticSamePadding(\n",
+       "     kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=[1, 1], ceil_mode=False\n",
+       "     (static_padding): ZeroPad2d((0, 0, 0, 1))\n",
+       "   )\n",
+       "   (_blocks): ModuleList(\n",
+       "     (0): MBConvBlock(\n",
+       "       (_depthwise_conv): Conv2dStaticSamePadding(\n",
+       "         8, 8, kernel_size=(5, 1), stride=(1, 1), groups=8, bias=False\n",
+       "         (static_padding): ZeroPad2d((0, 0, 2, 2))\n",
+       "       )\n",
+       "       (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "       (_se_reduce): Conv2dStaticSamePadding(\n",
+       "         8, 2, kernel_size=(1, 1), stride=(1, 1)\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_se_expand): Conv2dStaticSamePadding(\n",
+       "         2, 8, kernel_size=(1, 1), stride=(1, 1)\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_project_conv): Conv2dStaticSamePadding(\n",
+       "         8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_bn2): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "       (_swish): MemoryEfficientSwish()\n",
+       "     )\n",
+       "   )\n",
+       "   (_conv_head): Conv2dStaticSamePadding(\n",
+       "     8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "     (static_padding): Identity()\n",
+       "   )\n",
+       "   (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "   (_dropout): Dropout(p=0.7, inplace=False)\n",
+       "   (_fc): Linear(in_features=15840, out_features=1, bias=True)\n",
+       "   (_swish): MemoryEfficientSwish()\n",
+       " ),\n",
+       " 'state_dict': OrderedDict([('_conv_stem.weight',\n",
+       "               tensor([[[[-0.2219],\n",
+       "                         [-0.4197],\n",
+       "                         [ 0.3974]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.4642],\n",
+       "                         [ 0.0213],\n",
+       "                         [ 0.4852]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.1611],\n",
+       "                         [-0.0266],\n",
+       "                         [-0.2529]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[-0.5893],\n",
+       "                         [ 0.4024],\n",
+       "                         [ 0.3518]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.2247],\n",
+       "                         [ 0.1777],\n",
+       "                         [ 0.1444]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[-0.1371],\n",
+       "                         [ 0.2983],\n",
+       "                         [ 0.3501]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.4070],\n",
+       "                         [-0.3648],\n",
+       "                         [ 0.1564]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.2950],\n",
+       "                         [-0.1144],\n",
+       "                         [-0.0442]]]])),\n",
+       "              ('_bn0.weight',\n",
+       "               tensor([1.1501, 1.1684, 1.3061, 1.2520, 1.1763, 1.1265, 1.1788, 1.0600])),\n",
+       "              ('_bn0.bias',\n",
+       "               tensor([0.1123, 0.1197, 0.0233, 0.0785, 0.0163, 0.1060, 0.2230, 0.1400])),\n",
+       "              ('_bn0.running_mean',\n",
+       "               tensor([-0.2253,  0.8898, -0.1080,  0.1505,  0.5016,  0.4684,  0.1819,  0.1253])),\n",
+       "              ('_bn0.running_var',\n",
+       "               tensor([0.0144, 0.0717, 0.0038, 0.0213, 0.0218, 0.0239, 0.0111, 0.0041])),\n",
+       "              ('_bn0.num_batches_tracked', tensor(2856)),\n",
+       "              ('_blocks.0._depthwise_conv.weight',\n",
+       "               tensor([[[[ 0.2566],\n",
+       "                         [-0.1547],\n",
+       "                         [-0.4995],\n",
+       "                         [-0.4260],\n",
+       "                         [ 0.1059]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.2348],\n",
+       "                         [-0.3876],\n",
+       "                         [-0.2986],\n",
+       "                         [ 0.3064],\n",
+       "                         [ 0.3573]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[-0.1145],\n",
+       "                         [-0.1536],\n",
+       "                         [-0.1732],\n",
+       "                         [ 0.1974],\n",
+       "                         [ 0.0187]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.2808],\n",
+       "                         [ 0.2972],\n",
+       "                         [-0.4307],\n",
+       "                         [-0.3049],\n",
+       "                         [ 0.4325]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[-0.4085],\n",
+       "                         [ 0.1419],\n",
+       "                         [-0.0865],\n",
+       "                         [-0.0041],\n",
+       "                         [ 0.2004]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.3857],\n",
+       "                         [-0.0461],\n",
+       "                         [ 0.0605],\n",
+       "                         [ 0.0317],\n",
+       "                         [ 0.1978]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.1492],\n",
+       "                         [-0.3076],\n",
+       "                         [ 0.1330],\n",
+       "                         [ 0.2609],\n",
+       "                         [-0.0805]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.1794],\n",
+       "                         [ 0.0415],\n",
+       "                         [-0.0284],\n",
+       "                         [ 0.1662],\n",
+       "                         [-0.2655]]]])),\n",
+       "              ('_blocks.0._bn1.weight',\n",
+       "               tensor([1.0288, 1.0186, 0.9960, 1.0278, 1.0525, 1.0620, 1.0699, 0.9735])),\n",
+       "              ('_blocks.0._bn1.bias',\n",
+       "               tensor([0.1671, 0.0575, 0.0996, 0.0538, 0.1251, 0.0807, 0.2104, 0.0697])),\n",
+       "              ('_blocks.0._bn1.running_mean',\n",
+       "               tensor([-0.3346,  0.0831, -0.0849,  0.1397, -0.0430,  0.2396,  0.1021,  0.0433])),\n",
+       "              ('_blocks.0._bn1.running_var',\n",
+       "               tensor([0.3122, 0.1268, 0.0630, 0.4240, 0.0278, 0.0942, 0.1245, 0.0766])),\n",
+       "              ('_blocks.0._bn1.num_batches_tracked', tensor(2856)),\n",
+       "              ('_blocks.0._se_reduce.weight',\n",
+       "               tensor([[[[ 0.4492]],\n",
+       "               \n",
+       "                        [[ 0.1528]],\n",
+       "               \n",
+       "                        [[ 0.0406]],\n",
+       "               \n",
+       "                        [[ 0.0307]],\n",
+       "               \n",
+       "                        [[-0.0351]],\n",
+       "               \n",
+       "                        [[ 0.4482]],\n",
+       "               \n",
+       "                        [[ 0.2263]],\n",
+       "               \n",
+       "                        [[ 0.2292]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[-0.0610]],\n",
+       "               \n",
+       "                        [[ 0.5864]],\n",
+       "               \n",
+       "                        [[-0.1832]],\n",
+       "               \n",
+       "                        [[ 0.2124]],\n",
+       "               \n",
+       "                        [[ 0.2724]],\n",
+       "               \n",
+       "                        [[ 0.5372]],\n",
+       "               \n",
+       "                        [[ 0.5337]],\n",
+       "               \n",
+       "                        [[-0.0011]]]])),\n",
+       "              ('_blocks.0._se_reduce.bias', tensor([ 0.2626, -0.2148])),\n",
+       "              ('_blocks.0._se_expand.weight',\n",
+       "               tensor([[[[ 0.1844]],\n",
+       "               \n",
+       "                        [[ 0.1460]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[-0.0742]],\n",
+       "               \n",
+       "                        [[ 0.3871]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.3038]],\n",
+       "               \n",
+       "                        [[-0.4808]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[-0.6085]],\n",
+       "               \n",
+       "                        [[-0.2548]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[-0.6639]],\n",
+       "               \n",
+       "                        [[-0.6957]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.3321]],\n",
+       "               \n",
+       "                        [[ 0.4164]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[-0.7511]],\n",
+       "               \n",
+       "                        [[-0.9441]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[-0.2778]],\n",
+       "               \n",
+       "                        [[-0.1407]]]])),\n",
+       "              ('_blocks.0._se_expand.bias',\n",
+       "               tensor([-0.5998,  0.2574,  0.2413, -0.2627, -0.0965, -0.0101,  0.3204, -0.5279])),\n",
+       "              ('_blocks.0._project_conv.weight',\n",
+       "               tensor([[[[-0.3690]],\n",
+       "               \n",
+       "                        [[-0.1310]],\n",
+       "               \n",
+       "                        [[ 0.1829]],\n",
+       "               \n",
+       "                        [[-0.0026]],\n",
+       "               \n",
+       "                        [[-0.1824]],\n",
+       "               \n",
+       "                        [[ 0.1344]],\n",
+       "               \n",
+       "                        [[ 0.0261]],\n",
+       "               \n",
+       "                        [[-0.0739]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.2563]],\n",
+       "               \n",
+       "                        [[ 0.2295]],\n",
+       "               \n",
+       "                        [[-0.3668]],\n",
+       "               \n",
+       "                        [[-0.2978]],\n",
+       "               \n",
+       "                        [[-0.1745]],\n",
+       "               \n",
+       "                        [[ 0.1258]],\n",
+       "               \n",
+       "                        [[ 0.3694]],\n",
+       "               \n",
+       "                        [[ 0.0283]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.0648]],\n",
+       "               \n",
+       "                        [[-0.2387]],\n",
+       "               \n",
+       "                        [[ 0.1797]],\n",
+       "               \n",
+       "                        [[-0.2221]],\n",
+       "               \n",
+       "                        [[-0.2247]],\n",
+       "               \n",
+       "                        [[ 0.1591]],\n",
+       "               \n",
+       "                        [[ 0.2589]],\n",
+       "               \n",
+       "                        [[ 0.1436]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.2581]],\n",
+       "               \n",
+       "                        [[-0.1463]],\n",
+       "               \n",
+       "                        [[-0.0727]],\n",
+       "               \n",
+       "                        [[ 0.3045]],\n",
+       "               \n",
+       "                        [[ 0.4057]],\n",
+       "               \n",
+       "                        [[ 0.1405]],\n",
+       "               \n",
+       "                        [[-0.2834]],\n",
+       "               \n",
+       "                        [[ 0.0222]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.3131]],\n",
+       "               \n",
+       "                        [[-0.0660]],\n",
+       "               \n",
+       "                        [[-0.2989]],\n",
+       "               \n",
+       "                        [[-0.0643]],\n",
+       "               \n",
+       "                        [[ 0.0842]],\n",
+       "               \n",
+       "                        [[-0.2609]],\n",
+       "               \n",
+       "                        [[ 0.0173]],\n",
+       "               \n",
+       "                        [[ 0.1272]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.0419]],\n",
+       "               \n",
+       "                        [[ 0.3166]],\n",
+       "               \n",
+       "                        [[ 0.2722]],\n",
+       "               \n",
+       "                        [[ 0.0323]],\n",
+       "               \n",
+       "                        [[ 0.1228]],\n",
+       "               \n",
+       "                        [[ 0.0462]],\n",
+       "               \n",
+       "                        [[-0.1755]],\n",
+       "               \n",
+       "                        [[-0.2138]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.1194]],\n",
+       "               \n",
+       "                        [[-0.0329]],\n",
+       "               \n",
+       "                        [[-0.1728]],\n",
+       "               \n",
+       "                        [[ 0.2658]],\n",
+       "               \n",
+       "                        [[-0.3123]],\n",
+       "               \n",
+       "                        [[ 0.3451]],\n",
+       "               \n",
+       "                        [[-0.0749]],\n",
+       "               \n",
+       "                        [[-0.1774]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.1730]],\n",
+       "               \n",
+       "                        [[-0.3376]],\n",
+       "               \n",
+       "                        [[ 0.3677]],\n",
+       "               \n",
+       "                        [[-0.1899]],\n",
+       "               \n",
+       "                        [[ 0.1132]],\n",
+       "               \n",
+       "                        [[-0.1623]],\n",
+       "               \n",
+       "                        [[-0.2680]],\n",
+       "               \n",
+       "                        [[ 0.0390]]]])),\n",
+       "              ('_blocks.0._bn2.weight',\n",
+       "               tensor([1.0364, 0.9720, 0.9865, 0.9667, 0.9925, 1.0517, 1.0058, 1.0102])),\n",
+       "              ('_blocks.0._bn2.bias',\n",
+       "               tensor([ 0.0118,  0.0077, -0.0125, -0.0037,  0.0042,  0.0097,  0.0050, -0.0063])),\n",
+       "              ('_blocks.0._bn2.running_mean',\n",
+       "               tensor([-0.0214,  0.0264,  0.0321,  0.0366, -0.0282,  0.0415,  0.0036, -0.0209])),\n",
+       "              ('_blocks.0._bn2.running_var',\n",
+       "               tensor([0.0098, 0.0351, 0.0398, 0.0351, 0.0235, 0.0294, 0.0460, 0.0592])),\n",
+       "              ('_blocks.0._bn2.num_batches_tracked', tensor(2856)),\n",
+       "              ('_conv_head.weight',\n",
+       "               tensor([[[[-0.0669]],\n",
+       "               \n",
+       "                        [[ 0.2522]],\n",
+       "               \n",
+       "                        [[-0.1479]],\n",
+       "               \n",
+       "                        [[-0.2573]],\n",
+       "               \n",
+       "                        [[ 0.1993]],\n",
+       "               \n",
+       "                        [[-0.3531]],\n",
+       "               \n",
+       "                        [[-0.2546]],\n",
+       "               \n",
+       "                        [[-0.0795]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.1960]],\n",
+       "               \n",
+       "                        [[ 0.2518]],\n",
+       "               \n",
+       "                        [[ 0.1950]],\n",
+       "               \n",
+       "                        [[-0.2338]],\n",
+       "               \n",
+       "                        [[ 0.3294]],\n",
+       "               \n",
+       "                        [[ 0.1697]],\n",
+       "               \n",
+       "                        [[-0.1524]],\n",
+       "               \n",
+       "                        [[-0.2332]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.1111]],\n",
+       "               \n",
+       "                        [[-0.1626]],\n",
+       "               \n",
+       "                        [[-0.1817]],\n",
+       "               \n",
+       "                        [[ 0.1920]],\n",
+       "               \n",
+       "                        [[-0.1458]],\n",
+       "               \n",
+       "                        [[-0.0028]],\n",
+       "               \n",
+       "                        [[-0.1426]],\n",
+       "               \n",
+       "                        [[ 0.1355]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.1393]],\n",
+       "               \n",
+       "                        [[ 0.3351]],\n",
+       "               \n",
+       "                        [[-0.0991]],\n",
+       "               \n",
+       "                        [[-0.0262]],\n",
+       "               \n",
+       "                        [[ 0.2948]],\n",
+       "               \n",
+       "                        [[ 0.2487]],\n",
+       "               \n",
+       "                        [[ 0.1403]],\n",
+       "               \n",
+       "                        [[-0.3161]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[-0.0711]],\n",
+       "               \n",
+       "                        [[ 0.1313]],\n",
+       "               \n",
+       "                        [[ 0.1125]],\n",
+       "               \n",
+       "                        [[ 0.0406]],\n",
+       "               \n",
+       "                        [[-0.0345]],\n",
+       "               \n",
+       "                        [[-0.3978]],\n",
+       "               \n",
+       "                        [[ 0.3170]],\n",
+       "               \n",
+       "                        [[-0.0526]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[-0.3114]],\n",
+       "               \n",
+       "                        [[-0.0813]],\n",
+       "               \n",
+       "                        [[ 0.1744]],\n",
+       "               \n",
+       "                        [[ 0.1899]],\n",
+       "               \n",
+       "                        [[-0.1081]],\n",
+       "               \n",
+       "                        [[-0.0733]],\n",
+       "               \n",
+       "                        [[-0.2213]],\n",
+       "               \n",
+       "                        [[ 0.0740]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[-0.1832]],\n",
+       "               \n",
+       "                        [[ 0.1727]],\n",
+       "               \n",
+       "                        [[-0.1619]],\n",
+       "               \n",
+       "                        [[ 0.2191]],\n",
+       "               \n",
+       "                        [[ 0.2755]],\n",
+       "               \n",
+       "                        [[-0.2242]],\n",
+       "               \n",
+       "                        [[ 0.0585]],\n",
+       "               \n",
+       "                        [[ 0.1446]]],\n",
+       "               \n",
+       "               \n",
+       "                       [[[ 0.1458]],\n",
+       "               \n",
+       "                        [[ 0.0286]],\n",
+       "               \n",
+       "                        [[-0.2386]],\n",
+       "               \n",
+       "                        [[-0.1568]],\n",
+       "               \n",
+       "                        [[-0.0920]],\n",
+       "               \n",
+       "                        [[-0.1003]],\n",
+       "               \n",
+       "                        [[ 0.1615]],\n",
+       "               \n",
+       "                        [[ 0.1016]]]])),\n",
+       "              ('_bn1.weight',\n",
+       "               tensor([1.0736, 0.9171, 1.1093, 0.9017, 0.9667, 1.0479, 1.1473, 1.0540])),\n",
+       "              ('_bn1.bias',\n",
+       "               tensor([-0.0995, -0.0762, -0.1240, -0.0074, -0.0107, -0.1051, -0.0786, -0.0868])),\n",
+       "              ('_bn1.running_mean',\n",
+       "               tensor([ 0.0006,  0.0064, -0.0006,  0.0119, -0.0035, -0.0099, -0.0013,  0.0043])),\n",
+       "              ('_bn1.running_var',\n",
+       "               tensor([0.5606, 0.3092, 0.1374, 0.4120, 0.2739, 0.0738, 0.3514, 0.0285])),\n",
+       "              ('_bn1.num_batches_tracked', tensor(2856)),\n",
+       "              ('_fc.weight',\n",
+       "               tensor([[ 0.0005, -0.0074, -0.0403,  ...,  0.0171,  0.0141,  0.0220]])),\n",
+       "              ('_fc.bias', tensor([0.0095]))]),\n",
+       " 'optimizer': {'state': {0: {'step': 2856,\n",
+       "    'exp_avg': tensor([[[[ 0.0118],\n",
+       "              [ 0.0005],\n",
+       "              [ 0.0070]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 0.0006],\n",
+       "              [ 0.0027],\n",
+       "              [-0.0007]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 0.0108],\n",
+       "              [ 0.0082],\n",
+       "              [ 0.0064]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 0.0003],\n",
+       "              [-0.0004],\n",
+       "              [ 0.0009]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-0.0009],\n",
+       "              [ 0.0001],\n",
+       "              [ 0.0015]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-0.0053],\n",
+       "              [ 0.0025],\n",
+       "              [-0.0041]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 0.0072],\n",
+       "              [ 0.0082],\n",
+       "              [ 0.0004]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 0.0029],\n",
+       "              [ 0.0079],\n",
+       "              [-0.0015]]]]),\n",
+       "    'exp_avg_sq': tensor([[[[3.9561e-04],\n",
+       "              [2.9433e-04],\n",
+       "              [6.0602e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[3.9104e-05],\n",
+       "              [4.8522e-05],\n",
+       "              [3.5385e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[1.9404e-03],\n",
+       "              [1.8251e-03],\n",
+       "              [6.3564e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[2.1359e-05],\n",
+       "              [3.6891e-05],\n",
+       "              [3.8083e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[6.6898e-04],\n",
+       "              [7.4598e-04],\n",
+       "              [7.7449e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[3.8072e-04],\n",
+       "              [2.1656e-04],\n",
+       "              [1.1916e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[3.3735e-04],\n",
+       "              [4.4931e-04],\n",
+       "              [8.7067e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[6.3683e-05],\n",
+       "              [2.4472e-04],\n",
+       "              [2.6208e-04]]]])},\n",
+       "   1: {'step': 2856,\n",
+       "    'exp_avg': tensor([-1.1834e-04,  1.1196e-06, -3.5525e-04, -1.9603e-04,  1.2769e-04,\n",
+       "            -4.7237e-05, -3.0151e-04,  3.6306e-05]),\n",
+       "    'exp_avg_sq': tensor([2.7798e-06, 1.1577e-06, 1.6341e-06, 2.5364e-07, 5.7560e-06, 4.7967e-06,\n",
+       "            1.2656e-06, 4.1868e-07])},\n",
+       "   2: {'step': 2856,\n",
+       "    'exp_avg': tensor([ 0.0006,  0.0006, -0.0005, -0.0002,  0.0005,  0.0003, -0.0006, -0.0005]),\n",
+       "    'exp_avg_sq': tensor([1.4765e-05, 3.5675e-06, 1.5721e-05, 1.1685e-06, 1.0349e-04, 1.6890e-05,\n",
+       "            6.0731e-06, 1.9180e-06])},\n",
+       "   3: {'step': 2856,\n",
+       "    'exp_avg': tensor([[[[-3.2006e-03],\n",
+       "              [ 4.1536e-03],\n",
+       "              [ 1.9038e-04],\n",
+       "              [-3.6605e-03],\n",
+       "              [-1.5158e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 1.6303e-03],\n",
+       "              [ 1.3926e-03],\n",
+       "              [ 4.6458e-04],\n",
+       "              [ 1.9293e-03],\n",
+       "              [-8.3108e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-4.7997e-03],\n",
+       "              [ 1.0666e-03],\n",
+       "              [ 4.1406e-03],\n",
+       "              [ 1.0371e-03],\n",
+       "              [ 6.1629e-03]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 9.8053e-04],\n",
+       "              [-5.6776e-04],\n",
+       "              [ 6.6028e-04],\n",
+       "              [ 5.1617e-04],\n",
+       "              [ 7.7768e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-1.0104e-02],\n",
+       "              [-1.4045e-02],\n",
+       "              [-8.7443e-03],\n",
+       "              [-8.9563e-03],\n",
+       "              [-1.4723e-02]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-3.4595e-04],\n",
+       "              [-4.2823e-03],\n",
+       "              [-4.0619e-03],\n",
+       "              [ 6.6598e-04],\n",
+       "              [ 8.0053e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 4.5540e-03],\n",
+       "              [ 4.7096e-03],\n",
+       "              [ 6.7371e-04],\n",
+       "              [ 1.7526e-03],\n",
+       "              [-2.4256e-03]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 4.5798e-03],\n",
+       "              [-7.2487e-04],\n",
+       "              [-9.6922e-04],\n",
+       "              [-1.9425e-04],\n",
+       "              [ 2.9389e-03]]]]),\n",
+       "    'exp_avg_sq': tensor([[[[6.2598e-04],\n",
+       "              [1.7836e-04],\n",
+       "              [1.1787e-04],\n",
+       "              [1.1062e-04],\n",
+       "              [4.2221e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[1.2413e-04],\n",
+       "              [8.1631e-05],\n",
+       "              [9.8204e-05],\n",
+       "              [7.8790e-05],\n",
+       "              [6.7791e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[7.5566e-04],\n",
+       "              [6.3698e-04],\n",
+       "              [7.7405e-04],\n",
+       "              [1.4362e-03],\n",
+       "              [2.1436e-03]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[3.7000e-05],\n",
+       "              [4.4034e-05],\n",
+       "              [2.2612e-05],\n",
+       "              [2.4217e-05],\n",
+       "              [3.1306e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[1.3419e-03],\n",
+       "              [2.8389e-03],\n",
+       "              [3.4740e-03],\n",
+       "              [2.8529e-03],\n",
+       "              [4.1019e-03]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[9.9042e-05],\n",
+       "              [4.9986e-04],\n",
+       "              [4.3243e-04],\n",
+       "              [3.5027e-04],\n",
+       "              [3.1107e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[5.5727e-04],\n",
+       "              [4.8946e-04],\n",
+       "              [9.6423e-04],\n",
+       "              [4.9360e-04],\n",
+       "              [5.4775e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[2.7592e-04],\n",
+       "              [2.0781e-04],\n",
+       "              [1.5095e-04],\n",
+       "              [1.5321e-04],\n",
+       "              [1.2020e-04]]]])},\n",
+       "   4: {'step': 2856,\n",
+       "    'exp_avg': tensor([-0.0003, -0.0002, -0.0007,  0.0005, -0.0008, -0.0001, -0.0031,  0.0006]),\n",
+       "    'exp_avg_sq': tensor([1.1225e-04, 2.1333e-05, 1.1042e-04, 2.0036e-05, 1.5529e-04, 1.4068e-04,\n",
+       "            8.1843e-05, 2.6398e-05])},\n",
+       "   5: {'step': 2856,\n",
+       "    'exp_avg': tensor([-0.0009,  0.0012, -0.0028,  0.0006, -0.0003,  0.0007, -0.0002,  0.0003]),\n",
+       "    'exp_avg_sq': tensor([7.9720e-05, 1.0351e-05, 4.5844e-05, 5.4977e-06, 6.8632e-05, 6.0850e-05,\n",
+       "            2.0727e-05, 5.5903e-06])},\n",
+       "   6: {'step': 2856,\n",
+       "    'exp_avg': tensor([[[[-2.7662e-04]],\n",
+       "    \n",
+       "             [[-5.0580e-04]],\n",
+       "    \n",
+       "             [[-2.3880e-04]],\n",
+       "    \n",
+       "             [[-4.3469e-04]],\n",
+       "    \n",
+       "             [[-1.6905e-04]],\n",
+       "    \n",
+       "             [[-8.5880e-04]],\n",
+       "    \n",
+       "             [[-4.6771e-04]],\n",
+       "    \n",
+       "             [[-3.7756e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-1.8467e-04]],\n",
+       "    \n",
+       "             [[-4.4575e-04]],\n",
+       "    \n",
+       "             [[-1.5639e-04]],\n",
+       "    \n",
+       "             [[-3.7666e-04]],\n",
+       "    \n",
+       "             [[-5.4856e-05]],\n",
+       "    \n",
+       "             [[-8.2352e-04]],\n",
+       "    \n",
+       "             [[-3.9776e-04]],\n",
+       "    \n",
+       "             [[-3.1599e-04]]]]),\n",
+       "    'exp_avg_sq': tensor([[[[1.2938e-06]],\n",
+       "    \n",
+       "             [[1.1528e-06]],\n",
+       "    \n",
+       "             [[8.8368e-07]],\n",
+       "    \n",
+       "             [[1.2808e-06]],\n",
+       "    \n",
+       "             [[8.8113e-07]],\n",
+       "    \n",
+       "             [[2.8324e-06]],\n",
+       "    \n",
+       "             [[2.1082e-06]],\n",
+       "    \n",
+       "             [[8.7147e-07]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[9.2710e-07]],\n",
+       "    \n",
+       "             [[8.3367e-07]],\n",
+       "    \n",
+       "             [[6.2297e-07]],\n",
+       "    \n",
+       "             [[9.1790e-07]],\n",
+       "    \n",
+       "             [[6.0722e-07]],\n",
+       "    \n",
+       "             [[2.1272e-06]],\n",
+       "    \n",
+       "             [[1.5267e-06]],\n",
+       "    \n",
+       "             [[6.2512e-07]]]])},\n",
+       "   7: {'step': 2856,\n",
+       "    'exp_avg': tensor([-3.1333e-04, -6.4355e-05]),\n",
+       "    'exp_avg_sq': tensor([1.9458e-05, 1.3665e-05])},\n",
+       "   8: {'step': 2856,\n",
+       "    'exp_avg': tensor([[[[-1.6621e-05]],\n",
+       "    \n",
+       "             [[ 1.5384e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 1.6892e-05]],\n",
+       "    \n",
+       "             [[-9.9758e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-9.9467e-05]],\n",
+       "    \n",
+       "             [[ 6.4039e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 3.1641e-04]],\n",
+       "    \n",
+       "             [[ 1.7937e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 2.3301e-04]],\n",
+       "    \n",
+       "             [[ 3.5278e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 2.3511e-05]],\n",
+       "    \n",
+       "             [[-2.1581e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 1.7944e-04]],\n",
+       "    \n",
+       "             [[ 3.3848e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 5.5546e-04]],\n",
+       "    \n",
+       "             [[ 4.1479e-04]]]]),\n",
+       "    'exp_avg_sq': tensor([[[[7.9429e-06]],\n",
+       "    \n",
+       "             [[2.1177e-06]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[7.7962e-07]],\n",
+       "    \n",
+       "             [[2.9537e-07]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[2.9940e-06]],\n",
+       "    \n",
+       "             [[7.0802e-07]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[1.3406e-06]],\n",
+       "    \n",
+       "             [[2.9493e-07]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[5.5218e-06]],\n",
+       "    \n",
+       "             [[7.8809e-07]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[3.4332e-06]],\n",
+       "    \n",
+       "             [[8.1010e-07]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[4.3846e-06]],\n",
+       "    \n",
+       "             [[1.3378e-06]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[2.1659e-06]],\n",
+       "    \n",
+       "             [[6.7340e-07]]]])},\n",
+       "   9: {'step': 2856,\n",
+       "    'exp_avg': tensor([-0.0002,  0.0003, -0.0005,  0.0006, -0.0002,  0.0007, -0.0006,  0.0007]),\n",
+       "    'exp_avg_sq': tensor([4.4342e-05, 3.2435e-06, 1.7076e-05, 7.1548e-06, 4.1952e-05, 2.0365e-05,\n",
+       "            2.0836e-05, 9.4606e-06])},\n",
+       "   10: {'step': 2856,\n",
+       "    'exp_avg': tensor([[[[-0.0011]],\n",
+       "    \n",
+       "             [[ 0.0110]],\n",
+       "    \n",
+       "             [[ 0.0016]],\n",
+       "    \n",
+       "             [[ 0.0047]],\n",
+       "    \n",
+       "             [[ 0.0094]],\n",
+       "    \n",
+       "             [[ 0.0147]],\n",
+       "    \n",
+       "             [[ 0.0085]],\n",
+       "    \n",
+       "             [[-0.0038]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 0.0003]],\n",
+       "    \n",
+       "             [[ 0.0010]],\n",
+       "    \n",
+       "             [[ 0.0016]],\n",
+       "    \n",
+       "             [[ 0.0004]],\n",
+       "    \n",
+       "             [[-0.0033]],\n",
+       "    \n",
+       "             [[ 0.0027]],\n",
+       "    \n",
+       "             [[-0.0014]],\n",
+       "    \n",
+       "             [[ 0.0019]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-0.0007]],\n",
+       "    \n",
+       "             [[-0.0075]],\n",
+       "    \n",
+       "             [[-0.0012]],\n",
+       "    \n",
+       "             [[-0.0083]],\n",
+       "    \n",
+       "             [[-0.0018]],\n",
+       "    \n",
+       "             [[-0.0069]],\n",
+       "    \n",
+       "             [[-0.0092]],\n",
+       "    \n",
+       "             [[-0.0025]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-0.0005]],\n",
+       "    \n",
+       "             [[-0.0057]],\n",
+       "    \n",
+       "             [[-0.0038]],\n",
+       "    \n",
+       "             [[-0.0046]],\n",
+       "    \n",
+       "             [[ 0.0001]],\n",
+       "    \n",
+       "             [[-0.0095]],\n",
+       "    \n",
+       "             [[-0.0061]],\n",
+       "    \n",
+       "             [[-0.0013]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-0.0019]],\n",
+       "    \n",
+       "             [[-0.0003]],\n",
+       "    \n",
+       "             [[ 0.0003]],\n",
+       "    \n",
+       "             [[-0.0039]],\n",
+       "    \n",
+       "             [[-0.0027]],\n",
+       "    \n",
+       "             [[-0.0025]],\n",
+       "    \n",
+       "             [[-0.0050]],\n",
+       "    \n",
+       "             [[ 0.0004]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-0.0037]],\n",
+       "    \n",
+       "             [[-0.0037]],\n",
+       "    \n",
+       "             [[-0.0032]],\n",
+       "    \n",
+       "             [[-0.0062]],\n",
+       "    \n",
+       "             [[ 0.0024]],\n",
+       "    \n",
+       "             [[-0.0046]],\n",
+       "    \n",
+       "             [[-0.0050]],\n",
+       "    \n",
+       "             [[-0.0066]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 0.0003]],\n",
+       "    \n",
+       "             [[ 0.0024]],\n",
+       "    \n",
+       "             [[ 0.0041]],\n",
+       "    \n",
+       "             [[ 0.0045]],\n",
+       "    \n",
+       "             [[ 0.0002]],\n",
+       "    \n",
+       "             [[ 0.0017]],\n",
+       "    \n",
+       "             [[ 0.0061]],\n",
+       "    \n",
+       "             [[ 0.0023]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 0.0008]],\n",
+       "    \n",
+       "             [[-0.0015]],\n",
+       "    \n",
+       "             [[ 0.0016]],\n",
+       "    \n",
+       "             [[ 0.0037]],\n",
+       "    \n",
+       "             [[ 0.0002]],\n",
+       "    \n",
+       "             [[-0.0027]],\n",
+       "    \n",
+       "             [[ 0.0040]],\n",
+       "    \n",
+       "             [[ 0.0014]]]]),\n",
+       "    'exp_avg_sq': tensor([[[[4.0841e-04]],\n",
+       "    \n",
+       "             [[1.2621e-03]],\n",
+       "    \n",
+       "             [[9.1014e-04]],\n",
+       "    \n",
+       "             [[1.0339e-03]],\n",
+       "    \n",
+       "             [[4.6285e-04]],\n",
+       "    \n",
+       "             [[2.8778e-03]],\n",
+       "    \n",
+       "             [[1.4923e-03]],\n",
+       "    \n",
+       "             [[4.5772e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[3.4565e-05]],\n",
+       "    \n",
+       "             [[9.2877e-05]],\n",
+       "    \n",
+       "             [[1.0193e-04]],\n",
+       "    \n",
+       "             [[1.1191e-04]],\n",
+       "    \n",
+       "             [[6.8994e-05]],\n",
+       "    \n",
+       "             [[3.0663e-04]],\n",
+       "    \n",
+       "             [[1.1066e-04]],\n",
+       "    \n",
+       "             [[3.7786e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[5.5603e-05]],\n",
+       "    \n",
+       "             [[3.4943e-04]],\n",
+       "    \n",
+       "             [[1.9707e-04]],\n",
+       "    \n",
+       "             [[2.0437e-04]],\n",
+       "    \n",
+       "             [[5.1697e-05]],\n",
+       "    \n",
+       "             [[6.6036e-04]],\n",
+       "    \n",
+       "             [[1.7196e-04]],\n",
+       "    \n",
+       "             [[5.8979e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[8.4078e-05]],\n",
+       "    \n",
+       "             [[3.5985e-04]],\n",
+       "    \n",
+       "             [[2.7292e-04]],\n",
+       "    \n",
+       "             [[1.3301e-04]],\n",
+       "    \n",
+       "             [[1.0046e-04]],\n",
+       "    \n",
+       "             [[5.0625e-04]],\n",
+       "    \n",
+       "             [[3.7070e-04]],\n",
+       "    \n",
+       "             [[1.0791e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[6.9705e-05]],\n",
+       "    \n",
+       "             [[2.4760e-04]],\n",
+       "    \n",
+       "             [[1.3304e-04]],\n",
+       "    \n",
+       "             [[1.3308e-04]],\n",
+       "    \n",
+       "             [[8.3350e-05]],\n",
+       "    \n",
+       "             [[1.6508e-04]],\n",
+       "    \n",
+       "             [[2.2646e-04]],\n",
+       "    \n",
+       "             [[7.5083e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[5.5612e-05]],\n",
+       "    \n",
+       "             [[1.2074e-04]],\n",
+       "    \n",
+       "             [[1.3748e-04]],\n",
+       "    \n",
+       "             [[1.2747e-04]],\n",
+       "    \n",
+       "             [[5.1646e-05]],\n",
+       "    \n",
+       "             [[3.1728e-04]],\n",
+       "    \n",
+       "             [[2.3470e-04]],\n",
+       "    \n",
+       "             [[9.1340e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[4.1260e-05]],\n",
+       "    \n",
+       "             [[3.1857e-04]],\n",
+       "    \n",
+       "             [[3.4342e-04]],\n",
+       "    \n",
+       "             [[1.2727e-04]],\n",
+       "    \n",
+       "             [[1.4406e-04]],\n",
+       "    \n",
+       "             [[1.9559e-04]],\n",
+       "    \n",
+       "             [[3.8606e-04]],\n",
+       "    \n",
+       "             [[9.1749e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[1.9290e-05]],\n",
+       "    \n",
+       "             [[3.8367e-05]],\n",
+       "    \n",
+       "             [[5.4445e-05]],\n",
+       "    \n",
+       "             [[6.0909e-05]],\n",
+       "    \n",
+       "             [[2.7066e-05]],\n",
+       "    \n",
+       "             [[1.1495e-04]],\n",
+       "    \n",
+       "             [[5.7562e-05]],\n",
+       "    \n",
+       "             [[2.3076e-05]]]])},\n",
+       "   11: {'step': 2856,\n",
+       "    'exp_avg': tensor([ 2.2876e-04,  6.9741e-04, -1.2121e-03, -9.5486e-04, -4.0171e-06,\n",
+       "            -4.0203e-04,  3.8608e-03, -2.2497e-03]),\n",
+       "    'exp_avg_sq': tensor([7.0478e-05, 2.0298e-05, 7.8547e-05, 9.6405e-05, 4.6440e-05, 3.4957e-05,\n",
+       "            1.2140e-04, 3.4513e-05])},\n",
+       "   12: {'step': 2856,\n",
+       "    'exp_avg': tensor([ 1.9885e-09, -9.4927e-09,  4.4283e-09,  2.1050e-09, -1.0498e-08,\n",
+       "             2.5045e-09, -5.4988e-09,  2.0392e-09]),\n",
+       "    'exp_avg_sq': tensor([7.2940e-16, 3.1238e-16, 7.2652e-16, 6.4054e-16, 4.2074e-16, 3.5938e-16,\n",
+       "            6.3183e-16, 2.5401e-16])},\n",
+       "   13: {'step': 2856,\n",
+       "    'exp_avg': tensor([[[[ 2.3845e-04]],\n",
+       "    \n",
+       "             [[ 2.0772e-03]],\n",
+       "    \n",
+       "             [[ 1.5588e-03]],\n",
+       "    \n",
+       "             [[-1.3162e-03]],\n",
+       "    \n",
+       "             [[-1.2820e-03]],\n",
+       "    \n",
+       "             [[ 8.4009e-04]],\n",
+       "    \n",
+       "             [[ 5.0755e-04]],\n",
+       "    \n",
+       "             [[-7.6269e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-4.4636e-04]],\n",
+       "    \n",
+       "             [[-4.3349e-04]],\n",
+       "    \n",
+       "             [[-1.7387e-03]],\n",
+       "    \n",
+       "             [[-6.3005e-04]],\n",
+       "    \n",
+       "             [[ 1.4870e-04]],\n",
+       "    \n",
+       "             [[ 1.9163e-03]],\n",
+       "    \n",
+       "             [[-1.7325e-03]],\n",
+       "    \n",
+       "             [[ 1.1030e-03]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-6.1309e-03]],\n",
+       "    \n",
+       "             [[ 1.7206e-03]],\n",
+       "    \n",
+       "             [[-3.5641e-03]],\n",
+       "    \n",
+       "             [[-5.2116e-04]],\n",
+       "    \n",
+       "             [[ 5.4199e-03]],\n",
+       "    \n",
+       "             [[ 1.9711e-04]],\n",
+       "    \n",
+       "             [[-6.8599e-03]],\n",
+       "    \n",
+       "             [[ 1.5777e-03]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 2.4784e-03]],\n",
+       "    \n",
+       "             [[ 1.0207e-03]],\n",
+       "    \n",
+       "             [[ 1.5640e-03]],\n",
+       "    \n",
+       "             [[-1.8244e-03]],\n",
+       "    \n",
+       "             [[ 3.2196e-04]],\n",
+       "    \n",
+       "             [[-2.1089e-03]],\n",
+       "    \n",
+       "             [[ 1.2861e-03]],\n",
+       "    \n",
+       "             [[ 1.0415e-03]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-1.9611e-03]],\n",
+       "    \n",
+       "             [[ 1.7186e-04]],\n",
+       "    \n",
+       "             [[ 2.8945e-04]],\n",
+       "    \n",
+       "             [[ 2.2172e-03]],\n",
+       "    \n",
+       "             [[ 7.5504e-05]],\n",
+       "    \n",
+       "             [[ 1.4408e-03]],\n",
+       "    \n",
+       "             [[ 1.1843e-03]],\n",
+       "    \n",
+       "             [[ 1.5383e-03]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-1.6841e-04]],\n",
+       "    \n",
+       "             [[ 3.0464e-04]],\n",
+       "    \n",
+       "             [[ 2.8365e-03]],\n",
+       "    \n",
+       "             [[-3.7128e-03]],\n",
+       "    \n",
+       "             [[ 6.2940e-03]],\n",
+       "    \n",
+       "             [[-7.4093e-03]],\n",
+       "    \n",
+       "             [[-9.3237e-06]],\n",
+       "    \n",
+       "             [[ 3.4627e-03]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[-4.6296e-04]],\n",
+       "    \n",
+       "             [[-6.8864e-04]],\n",
+       "    \n",
+       "             [[ 2.3916e-04]],\n",
+       "    \n",
+       "             [[-1.9112e-03]],\n",
+       "    \n",
+       "             [[ 1.1289e-03]],\n",
+       "    \n",
+       "             [[-1.8046e-03]],\n",
+       "    \n",
+       "             [[-1.1186e-03]],\n",
+       "    \n",
+       "             [[-1.1223e-03]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[ 2.7399e-03]],\n",
+       "    \n",
+       "             [[ 1.1259e-02]],\n",
+       "    \n",
+       "             [[ 6.7363e-03]],\n",
+       "    \n",
+       "             [[ 2.0587e-03]],\n",
+       "    \n",
+       "             [[-1.3795e-02]],\n",
+       "    \n",
+       "             [[ 3.0005e-03]],\n",
+       "    \n",
+       "             [[ 1.4097e-02]],\n",
+       "    \n",
+       "             [[-1.9418e-02]]]]),\n",
+       "    'exp_avg_sq': tensor([[[[5.6743e-05]],\n",
+       "    \n",
+       "             [[6.7635e-05]],\n",
+       "    \n",
+       "             [[8.5339e-05]],\n",
+       "    \n",
+       "             [[3.1450e-05]],\n",
+       "    \n",
+       "             [[5.0329e-05]],\n",
+       "    \n",
+       "             [[3.0950e-05]],\n",
+       "    \n",
+       "             [[2.6591e-05]],\n",
+       "    \n",
+       "             [[6.7917e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[9.1895e-05]],\n",
+       "    \n",
+       "             [[4.3772e-05]],\n",
+       "    \n",
+       "             [[7.8174e-05]],\n",
+       "    \n",
+       "             [[1.2708e-05]],\n",
+       "    \n",
+       "             [[7.4189e-05]],\n",
+       "    \n",
+       "             [[1.1495e-04]],\n",
+       "    \n",
+       "             [[6.0382e-05]],\n",
+       "    \n",
+       "             [[7.4934e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[3.9927e-04]],\n",
+       "    \n",
+       "             [[4.7498e-05]],\n",
+       "    \n",
+       "             [[2.5058e-04]],\n",
+       "    \n",
+       "             [[3.7154e-04]],\n",
+       "    \n",
+       "             [[4.5818e-04]],\n",
+       "    \n",
+       "             [[3.0388e-04]],\n",
+       "    \n",
+       "             [[4.8577e-04]],\n",
+       "    \n",
+       "             [[3.3840e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[9.5446e-05]],\n",
+       "    \n",
+       "             [[1.6772e-05]],\n",
+       "    \n",
+       "             [[5.7599e-05]],\n",
+       "    \n",
+       "             [[8.1472e-05]],\n",
+       "    \n",
+       "             [[9.3391e-05]],\n",
+       "    \n",
+       "             [[4.4937e-05]],\n",
+       "    \n",
+       "             [[8.4777e-05]],\n",
+       "    \n",
+       "             [[1.9430e-05]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[1.2148e-04]],\n",
+       "    \n",
+       "             [[8.5878e-05]],\n",
+       "    \n",
+       "             [[7.9660e-05]],\n",
+       "    \n",
+       "             [[1.5023e-04]],\n",
+       "    \n",
+       "             [[2.3295e-04]],\n",
+       "    \n",
+       "             [[1.1555e-04]],\n",
+       "    \n",
+       "             [[1.2776e-04]],\n",
+       "    \n",
+       "             [[1.0094e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[6.3493e-04]],\n",
+       "    \n",
+       "             [[6.5023e-04]],\n",
+       "    \n",
+       "             [[5.7117e-04]],\n",
+       "    \n",
+       "             [[4.5963e-04]],\n",
+       "    \n",
+       "             [[5.3550e-04]],\n",
+       "    \n",
+       "             [[7.1203e-04]],\n",
+       "    \n",
+       "             [[3.8433e-04]],\n",
+       "    \n",
+       "             [[7.6228e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[4.7283e-05]],\n",
+       "    \n",
+       "             [[1.6134e-04]],\n",
+       "    \n",
+       "             [[9.5311e-05]],\n",
+       "    \n",
+       "             [[2.3215e-04]],\n",
+       "    \n",
+       "             [[7.2813e-05]],\n",
+       "    \n",
+       "             [[7.7996e-05]],\n",
+       "    \n",
+       "             [[2.5557e-04]],\n",
+       "    \n",
+       "             [[1.6766e-04]]],\n",
+       "    \n",
+       "    \n",
+       "            [[[4.8358e-04]],\n",
+       "    \n",
+       "             [[1.5630e-03]],\n",
+       "    \n",
+       "             [[7.9488e-04]],\n",
+       "    \n",
+       "             [[1.0770e-03]],\n",
+       "    \n",
+       "             [[1.2558e-03]],\n",
+       "    \n",
+       "             [[1.0575e-03]],\n",
+       "    \n",
+       "             [[1.2774e-03]],\n",
+       "    \n",
+       "             [[1.6264e-03]]]])},\n",
+       "   14: {'step': 2856,\n",
+       "    'exp_avg': tensor([-0.0025, -0.0025, -0.0014,  0.0008,  0.0010, -0.0044, -0.0011,  0.0018]),\n",
+       "    'exp_avg_sq': tensor([3.8689e-05, 3.3381e-05, 4.3376e-05, 4.6667e-05, 5.4333e-05, 7.0278e-05,\n",
+       "            4.3675e-05, 6.8123e-05])},\n",
+       "   15: {'step': 2856,\n",
+       "    'exp_avg': tensor([-0.0011,  0.0022,  0.0029,  0.0092,  0.0081,  0.0002,  0.0026,  0.0065]),\n",
+       "    'exp_avg_sq': tensor([8.6317e-05, 1.0236e-04, 1.0477e-04, 5.4282e-04, 6.3644e-04, 3.2021e-04,\n",
+       "            3.4920e-04, 2.6240e-04])},\n",
+       "   16: {'step': 2856,\n",
+       "    'exp_avg': tensor([[ 0.0008,  0.0006,  0.0018,  ..., -0.0004,  0.0003, -0.0009]]),\n",
+       "    'exp_avg_sq': tensor([[2.3162e-05, 1.8886e-05, 2.1638e-05,  ..., 1.3613e-05, 1.4928e-05,\n",
+       "             1.4566e-05]])},\n",
+       "   17: {'step': 2856,\n",
+       "    'exp_avg': tensor([-0.0032]),\n",
+       "    'exp_avg_sq': tensor([0.0002])}},\n",
+       "  'param_groups': [{'lr': 2.3263051398720685e-06,\n",
+       "    'betas': (0.9, 0.999),\n",
+       "    'eps': 1e-08,\n",
+       "    'weight_decay': 0,\n",
+       "    'buffer': [[2850, 1649.573661948345, 0.8814982883453465],\n",
+       "     [2851, 1649.822010771077, 0.881591904744738],\n",
+       "     [2852, 1650.0702033134994, 0.8816854504411451],\n",
+       "     [2853, 1650.3182396533077, 0.8817789254849911],\n",
+       "     [2854, 1650.5661198681787, 0.8818723299266661],\n",
+       "     [2855, 1650.8138440357716, 0.8819656638165265],\n",
+       "     [2856, 1651.061412233727, 0.8820589272048958],\n",
+       "     [2847, 1648.82767702107, 0.8812170144245286],\n",
+       "     [2848, 1649.0764951510819, 0.8813108432357489],\n",
+       "     [2849, 1649.3251567675907, 0.8814046011925136]],\n",
+       "    'params': [0,\n",
+       "     1,\n",
+       "     2,\n",
+       "     3,\n",
+       "     4,\n",
+       "     5,\n",
+       "     6,\n",
+       "     7,\n",
+       "     8,\n",
+       "     9,\n",
+       "     10,\n",
+       "     11,\n",
+       "     12,\n",
+       "     13,\n",
+       "     14,\n",
+       "     15,\n",
+       "     16,\n",
+       "     17]}]}}"
+      ]
+     },
+     "execution_count": 16,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "checkpoint = torch.load(model_path_list[0], map_location='cpu')\n",
+    "checkpoint"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
    "metadata": {},
    "outputs": [],
    "source": [
     "import os \n",
     "def load_multiple_model(filedir, datatype):\n",
     "    model_path_list = [f'{filedir}/{file}' for file in os.listdir(filedir) if \"best\" in file]\n",
-    "    print(model_path_list)\n",
+    "    # print(model_path_list)\n",
     "    globals()['model_list_' + datatype] = []\n",
     "    # estimators = []\n",
     "    for i in range(len(model_path_list)):\n",
@@ -110,20 +1629,13 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "['/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn1_1846_bs1024_weight0/3fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn1_1846_bs1024_weight0/0fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn1_1846_bs1024_weight0/1fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn1_1846_bs1024_weight0/4fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn1_1846_bs1024_weight0/2fold_best_model.pth']\n"
-     ]
-    },
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "f7cc79c6a30f4ee085d0509620ebb4a9",
+       "model_id": "3386473b3099448ea43b3aa25a2e8aa2",
        "version_major": 2,
        "version_minor": 0
       },
@@ -137,7 +1649,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "59f067efeadb49fc82e1f4a2b1f0ec05",
+       "model_id": "bf9de70f1e8147f8ae77a2e5e8184c27",
        "version_major": 2,
        "version_minor": 0
       },
@@ -151,7 +1663,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "bedb1e2274194115af9f7c1c96fd5b90",
+       "model_id": "ea0c69a5d2bf4efda8f593de0d584dac",
        "version_major": 2,
        "version_minor": 0
       },
@@ -165,7 +1677,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "b39fcc92e8cf4851bb0296e613da4824",
+       "model_id": "98539ff9e1f24d0b9c9e29d486baa89e",
        "version_major": 2,
        "version_minor": 0
       },
@@ -179,7 +1691,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "0cd280b00a5d426e9bc916b73a3d74e7",
+       "model_id": "d500f27b842a490eb7093f0bbafa0bca",
        "version_major": 2,
        "version_minor": 0
       },
@@ -190,17 +1702,10 @@
      "metadata": {},
      "output_type": "display_data"
     },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "['/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn2_1846_bs1024_weight0/3fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn2_1846_bs1024_weight0/0fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn2_1846_bs1024_weight0/1fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn2_1846_bs1024_weight0/4fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn2_1846_bs1024_weight0/2fold_best_model.pth']\n"
-     ]
-    },
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "b4e687cc33784bcc9ab46cda7b741c83",
+       "model_id": "1dde04b7191048fb96c89bbd981cff5b",
        "version_major": 2,
        "version_minor": 0
       },
@@ -214,7 +1719,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "3dce7de670f946e7b63f2feccd87613f",
+       "model_id": "562978669ce04c118960fe49cdc4f25e",
        "version_major": 2,
        "version_minor": 0
       },
@@ -228,7 +1733,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "0368726f5c084bd5895ca6e11e846d80",
+       "model_id": "b790fbfecf884ec8b4ff2c0eeb6b08ef",
        "version_major": 2,
        "version_minor": 0
       },
@@ -242,7 +1747,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "cb04243536e74bf68c168ac100ade935",
+       "model_id": "7c6613bb1b7e4f0fa042eab0f6923916",
        "version_major": 2,
        "version_minor": 0
       },
@@ -256,7 +1761,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "cd224d97830846adbcda6f9a17df2065",
+       "model_id": "2b3ee06c4c284467a2f6c0bd37e7a327",
        "version_major": 2,
        "version_minor": 0
       },
@@ -267,17 +1772,10 @@
      "metadata": {},
      "output_type": "display_data"
     },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "['/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn3_1846_bs1024_weight0/3fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn3_1846_bs1024_weight0/0fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn3_1846_bs1024_weight0/1fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn3_1846_bs1024_weight0/4fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn3_1846_bs1024_weight0/2fold_best_model.pth']\n"
-     ]
-    },
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "b9206f8fde2241f094c50d52a0ef9d39",
+       "model_id": "434f1da565744213b3f6fa797a7c3fe5",
        "version_major": 2,
        "version_minor": 0
       },
@@ -291,7 +1789,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "ba3ca9a02e524621b4e4ca6347b2e777",
+       "model_id": "a8fb2d9eba004502a9ae7af5b3576a06",
        "version_major": 2,
        "version_minor": 0
       },
@@ -305,7 +1803,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "975bbcd85d954ddd95daa92c61e829a1",
+       "model_id": "1fe2af8bb792407681784d992194a65b",
        "version_major": 2,
        "version_minor": 0
       },
@@ -319,7 +1817,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "b4c6f7b5e5e34878a303e706c4e2be7f",
+       "model_id": "aa47d6dfd6584215aa288a307524853a",
        "version_major": 2,
        "version_minor": 0
       },
@@ -333,7 +1831,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "c5aa1a9a73f44a02adb8907b70ff9611",
+       "model_id": "e6baae694f51455ca1a2feab86ed55d6",
        "version_major": 2,
        "version_minor": 0
       },
@@ -344,17 +1842,10 @@
      "metadata": {},
      "output_type": "display_data"
     },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "['/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn4_1846_bs1024_weight0/3fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn4_1846_bs1024_weight0/0fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn4_1846_bs1024_weight0/1fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn4_1846_bs1024_weight0/4fold_best_model.pth', '/home/hb/python/efficientnet_kincnn/saved_model/0517/DeepPP_kincnn4_1846_bs1024_weight0/2fold_best_model.pth']\n"
-     ]
-    },
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "619811cb4a044401b059780d4f7c7010",
+       "model_id": "396801fbeed6454a9ce6b2a7f83a5a61",
        "version_major": 2,
        "version_minor": 0
       },
@@ -368,7 +1859,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "9dad365bdd5e41d897a69d1d22cf2495",
+       "model_id": "8a75ac48c50643a9b7fff388db034217",
        "version_major": 2,
        "version_minor": 0
       },
@@ -382,7 +1873,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "a59394447917403585cef6f49a38121a",
+       "model_id": "995406a09bc3495f9efe7c8468b2d1fa",
        "version_major": 2,
        "version_minor": 0
       },
@@ -396,7 +1887,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "7d4088bf23a44dd89ab42658900f18d1",
+       "model_id": "3eab9595449d4977b2fb62099979d6d5",
        "version_major": 2,
        "version_minor": 0
       },
@@ -410,7 +1901,7 @@
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
-       "model_id": "6b871f4d70854ccf807793b179c3229b",
+       "model_id": "ec68588f8ae6426d877df18891e3f6ba",
        "version_major": 2,
        "version_minor": 0
       },
@@ -420,15 +1911,26 @@
      },
      "metadata": {},
      "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/plain": [
+       "[0.8208, 0.834, 0.8339, 0.8367]"
+      ]
+     },
+     "execution_count": 3,
+     "metadata": {},
+     "output_type": "execute_result"
     }
    ],
    "source": [
     "import re\n",
     "df_test = pd.read_pickle(\"/home/hb/python/phospho/data/required/0308_final_train_test/test1.pkl\")\n",
-    "time = 1846\n",
-    "save_dir = '/home/hb/python/efficientnet_kincnn/saved_model/0517'\n",
+    "time = 1251\n",
+    "save_dir = '/home/hb/python/efficientnet_kincnn/saved_model/0518'\n",
     "\n",
     "filedir_list = [f'{save_dir}/DeepPP_kincnn{x}_{time}_bs1024_weight0' for x in range(1, 5)]\n",
+    "# filedir_list = [f'{save_dir}/DeepPP_kincnn{1}_{1100}_bs1024_weight0']\n",
     "model_list = []\n",
     "auc_list = []\n",
     "for filedir in filedir_list:\n",
@@ -436,52 +1938,312 @@
     "    model = load_multiple_model(filedir, filedir_found)\n",
     "    model_list.append(model)\n",
     "    auc = ensemble(model, df_test)\n",
-    "    auc_list.append(round(auc, 4))"
+    "    auc_list.append(round(auc, 4))\n",
+    "auc_list"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 95,
+   "execution_count": 9,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "model = load_multiple_model(filedir, filedir_found)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 10,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
-       "'0517/DeepPP_kincnn1_1846_bs1024_weight0'"
+       "[EfficientNet(\n",
+       "   (_conv_stem): Conv2dStaticSamePadding(\n",
+       "     1, 8, kernel_size=(3, 1), stride=(1, 1), bias=False\n",
+       "     (static_padding): ZeroPad2d((0, 0, 1, 1))\n",
+       "   )\n",
+       "   (_bn0): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "   (_max_pooling): MaxPool2dStaticSamePadding(\n",
+       "     kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=[1, 1], ceil_mode=False\n",
+       "     (static_padding): ZeroPad2d((0, 0, 0, 1))\n",
+       "   )\n",
+       "   (_blocks): ModuleList(\n",
+       "     (0): MBConvBlock(\n",
+       "       (_depthwise_conv): Conv2dStaticSamePadding(\n",
+       "         8, 8, kernel_size=(5, 1), stride=(1, 1), groups=8, bias=False\n",
+       "         (static_padding): ZeroPad2d((0, 0, 2, 2))\n",
+       "       )\n",
+       "       (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "       (_se_reduce): Conv2dStaticSamePadding(\n",
+       "         8, 2, kernel_size=(1, 1), stride=(1, 1)\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_se_expand): Conv2dStaticSamePadding(\n",
+       "         2, 8, kernel_size=(1, 1), stride=(1, 1)\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_project_conv): Conv2dStaticSamePadding(\n",
+       "         8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_bn2): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "       (_swish): MemoryEfficientSwish()\n",
+       "     )\n",
+       "   )\n",
+       "   (_conv_head): Conv2dStaticSamePadding(\n",
+       "     8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "     (static_padding): Identity()\n",
+       "   )\n",
+       "   (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "   (_dropout): Dropout(p=0.7, inplace=False)\n",
+       "   (_fc): Linear(in_features=15840, out_features=1, bias=True)\n",
+       "   (_swish): MemoryEfficientSwish()\n",
+       " ),\n",
+       " EfficientNet(\n",
+       "   (_conv_stem): Conv2dStaticSamePadding(\n",
+       "     1, 8, kernel_size=(3, 1), stride=(1, 1), bias=False\n",
+       "     (static_padding): ZeroPad2d((0, 0, 1, 1))\n",
+       "   )\n",
+       "   (_bn0): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "   (_max_pooling): MaxPool2dStaticSamePadding(\n",
+       "     kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=[1, 1], ceil_mode=False\n",
+       "     (static_padding): ZeroPad2d((0, 0, 0, 1))\n",
+       "   )\n",
+       "   (_blocks): ModuleList(\n",
+       "     (0): MBConvBlock(\n",
+       "       (_depthwise_conv): Conv2dStaticSamePadding(\n",
+       "         8, 8, kernel_size=(5, 1), stride=(1, 1), groups=8, bias=False\n",
+       "         (static_padding): ZeroPad2d((0, 0, 2, 2))\n",
+       "       )\n",
+       "       (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "       (_se_reduce): Conv2dStaticSamePadding(\n",
+       "         8, 2, kernel_size=(1, 1), stride=(1, 1)\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_se_expand): Conv2dStaticSamePadding(\n",
+       "         2, 8, kernel_size=(1, 1), stride=(1, 1)\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_project_conv): Conv2dStaticSamePadding(\n",
+       "         8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_bn2): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "       (_swish): MemoryEfficientSwish()\n",
+       "     )\n",
+       "   )\n",
+       "   (_conv_head): Conv2dStaticSamePadding(\n",
+       "     8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "     (static_padding): Identity()\n",
+       "   )\n",
+       "   (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "   (_dropout): Dropout(p=0.7, inplace=False)\n",
+       "   (_fc): Linear(in_features=15840, out_features=1, bias=True)\n",
+       "   (_swish): MemoryEfficientSwish()\n",
+       " ),\n",
+       " EfficientNet(\n",
+       "   (_conv_stem): Conv2dStaticSamePadding(\n",
+       "     1, 8, kernel_size=(3, 1), stride=(1, 1), bias=False\n",
+       "     (static_padding): ZeroPad2d((0, 0, 1, 1))\n",
+       "   )\n",
+       "   (_bn0): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "   (_max_pooling): MaxPool2dStaticSamePadding(\n",
+       "     kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=[1, 1], ceil_mode=False\n",
+       "     (static_padding): ZeroPad2d((0, 0, 0, 1))\n",
+       "   )\n",
+       "   (_blocks): ModuleList(\n",
+       "     (0): MBConvBlock(\n",
+       "       (_depthwise_conv): Conv2dStaticSamePadding(\n",
+       "         8, 8, kernel_size=(5, 1), stride=(1, 1), groups=8, bias=False\n",
+       "         (static_padding): ZeroPad2d((0, 0, 2, 2))\n",
+       "       )\n",
+       "       (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "       (_se_reduce): Conv2dStaticSamePadding(\n",
+       "         8, 2, kernel_size=(1, 1), stride=(1, 1)\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_se_expand): Conv2dStaticSamePadding(\n",
+       "         2, 8, kernel_size=(1, 1), stride=(1, 1)\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_project_conv): Conv2dStaticSamePadding(\n",
+       "         8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_bn2): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "       (_swish): MemoryEfficientSwish()\n",
+       "     )\n",
+       "   )\n",
+       "   (_conv_head): Conv2dStaticSamePadding(\n",
+       "     8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "     (static_padding): Identity()\n",
+       "   )\n",
+       "   (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "   (_dropout): Dropout(p=0.7, inplace=False)\n",
+       "   (_fc): Linear(in_features=15840, out_features=1, bias=True)\n",
+       "   (_swish): MemoryEfficientSwish()\n",
+       " ),\n",
+       " EfficientNet(\n",
+       "   (_conv_stem): Conv2dStaticSamePadding(\n",
+       "     1, 8, kernel_size=(3, 1), stride=(1, 1), bias=False\n",
+       "     (static_padding): ZeroPad2d((0, 0, 1, 1))\n",
+       "   )\n",
+       "   (_bn0): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "   (_max_pooling): MaxPool2dStaticSamePadding(\n",
+       "     kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=[1, 1], ceil_mode=False\n",
+       "     (static_padding): ZeroPad2d((0, 0, 0, 1))\n",
+       "   )\n",
+       "   (_blocks): ModuleList(\n",
+       "     (0): MBConvBlock(\n",
+       "       (_depthwise_conv): Conv2dStaticSamePadding(\n",
+       "         8, 8, kernel_size=(5, 1), stride=(1, 1), groups=8, bias=False\n",
+       "         (static_padding): ZeroPad2d((0, 0, 2, 2))\n",
+       "       )\n",
+       "       (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "       (_se_reduce): Conv2dStaticSamePadding(\n",
+       "         8, 2, kernel_size=(1, 1), stride=(1, 1)\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_se_expand): Conv2dStaticSamePadding(\n",
+       "         2, 8, kernel_size=(1, 1), stride=(1, 1)\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_project_conv): Conv2dStaticSamePadding(\n",
+       "         8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_bn2): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "       (_swish): MemoryEfficientSwish()\n",
+       "     )\n",
+       "   )\n",
+       "   (_conv_head): Conv2dStaticSamePadding(\n",
+       "     8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "     (static_padding): Identity()\n",
+       "   )\n",
+       "   (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "   (_dropout): Dropout(p=0.7, inplace=False)\n",
+       "   (_fc): Linear(in_features=15840, out_features=1, bias=True)\n",
+       "   (_swish): MemoryEfficientSwish()\n",
+       " ),\n",
+       " EfficientNet(\n",
+       "   (_conv_stem): Conv2dStaticSamePadding(\n",
+       "     1, 8, kernel_size=(3, 1), stride=(1, 1), bias=False\n",
+       "     (static_padding): ZeroPad2d((0, 0, 1, 1))\n",
+       "   )\n",
+       "   (_bn0): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "   (_max_pooling): MaxPool2dStaticSamePadding(\n",
+       "     kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=[1, 1], ceil_mode=False\n",
+       "     (static_padding): ZeroPad2d((0, 0, 0, 1))\n",
+       "   )\n",
+       "   (_blocks): ModuleList(\n",
+       "     (0): MBConvBlock(\n",
+       "       (_depthwise_conv): Conv2dStaticSamePadding(\n",
+       "         8, 8, kernel_size=(5, 1), stride=(1, 1), groups=8, bias=False\n",
+       "         (static_padding): ZeroPad2d((0, 0, 2, 2))\n",
+       "       )\n",
+       "       (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "       (_se_reduce): Conv2dStaticSamePadding(\n",
+       "         8, 2, kernel_size=(1, 1), stride=(1, 1)\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_se_expand): Conv2dStaticSamePadding(\n",
+       "         2, 8, kernel_size=(1, 1), stride=(1, 1)\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_project_conv): Conv2dStaticSamePadding(\n",
+       "         8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "         (static_padding): Identity()\n",
+       "       )\n",
+       "       (_bn2): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "       (_swish): MemoryEfficientSwish()\n",
+       "     )\n",
+       "   )\n",
+       "   (_conv_head): Conv2dStaticSamePadding(\n",
+       "     8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "     (static_padding): Identity()\n",
+       "   )\n",
+       "   (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "   (_dropout): Dropout(p=0.7, inplace=False)\n",
+       "   (_fc): Linear(in_features=15840, out_features=1, bias=True)\n",
+       "   (_swish): MemoryEfficientSwish()\n",
+       " )]"
       ]
      },
-     "execution_count": 95,
+     "execution_count": 10,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
-    "'/'.join(filedir_list[0].split('/')[-2:])"
+    "model"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 78,
+   "execution_count": 8,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
-       "4"
+       "EfficientNet(\n",
+       "  (_conv_stem): Conv2dStaticSamePadding(\n",
+       "    1, 8, kernel_size=(3, 1), stride=(1, 1), bias=False\n",
+       "    (static_padding): ZeroPad2d((0, 0, 1, 1))\n",
+       "  )\n",
+       "  (_bn0): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "  (_max_pooling): MaxPool2dStaticSamePadding(\n",
+       "    kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=[1, 1], ceil_mode=False\n",
+       "    (static_padding): ZeroPad2d((0, 0, 0, 1))\n",
+       "  )\n",
+       "  (_blocks): ModuleList(\n",
+       "    (0): MBConvBlock(\n",
+       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
+       "        8, 8, kernel_size=(5, 1), stride=(1, 1), groups=8, bias=False\n",
+       "        (static_padding): ZeroPad2d((0, 0, 2, 2))\n",
+       "      )\n",
+       "      (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "      (_se_reduce): Conv2dStaticSamePadding(\n",
+       "        8, 2, kernel_size=(1, 1), stride=(1, 1)\n",
+       "        (static_padding): Identity()\n",
+       "      )\n",
+       "      (_se_expand): Conv2dStaticSamePadding(\n",
+       "        2, 8, kernel_size=(1, 1), stride=(1, 1)\n",
+       "        (static_padding): Identity()\n",
+       "      )\n",
+       "      (_project_conv): Conv2dStaticSamePadding(\n",
+       "        8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "        (static_padding): Identity()\n",
+       "      )\n",
+       "      (_bn2): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "      (_swish): MemoryEfficientSwish()\n",
+       "    )\n",
+       "  )\n",
+       "  (_conv_head): Conv2dStaticSamePadding(\n",
+       "    8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "    (static_padding): Identity()\n",
+       "  )\n",
+       "  (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "  (_dropout): Dropout(p=0.7, inplace=False)\n",
+       "  (_fc): Linear(in_features=15840, out_features=1, bias=True)\n",
+       "  (_swish): MemoryEfficientSwish()\n",
+       ")"
       ]
      },
-     "execution_count": 78,
+     "execution_count": 8,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
-    "len(model_list)"
+    "model_list[0][0]"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 98,
+   "execution_count": 4,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -489,12 +2251,14 @@
     "\n",
     "warnings.filterwarnings(action='ignore') \n",
     "\n",
-    "df = pd.read_csv('/home/hb/python/efficientnet_kincnn/model_info.csv', index_col=False)\n",
-    "del df['Unnamed: 0']\n",
+    "df = pd.read_excel('/home/hb/python/efficientnet_kincnn/model_info.xlsx', engine='openpyxl')\n",
     "\n",
-    "for model, auc, model_path in zip(model_list, auc_list, filedir_list):\n",
+    "\n",
+    "for idx, model, auc, model_path in zip(range(len(model_list)), model_list, auc_list, filedir_list):\n",
     "    _model = model[0]\n",
     "    model_params_dict = {}\n",
+    "    model_params_dict['data'] = f'train{idx+1}'\n",
+    "    model_params_dict['AUC'] = auc\n",
     "    model_params_dict['conv_stem_kernel_size'] = _model._conv_stem.kernel_size\n",
     "    model_params_dict['conv_stem_stride_size'] = _model._conv_stem.stride\n",
     "    model_params_dict['conv_stem_pooling'] = bool(_model._max_pooling)\n",
@@ -508,15 +2272,14 @@
     "        model_params_dict[f'MBconv{i}_kernel_size'] = _model._blocks[i]._depthwise_conv.kernel_size\n",
     "        model_params_dict[f'MBconv{i}_stride'] = _model._blocks[i]._depthwise_conv.stride\n",
     "    model_params_dict['last_features'] = _model._fc.in_features\n",
-    "    model_params_dict['AUC'] = auc\n",
     "    model_params_dict['model_path'] = '/'.join(model_path.split('/')[-2:])\n",
     "    df = df.append(model_params_dict, ignore_index=True)\n",
-    "df.to_csv(\"/home/hb/python/efficientnet_kincnn/model_info.csv\")"
+    "df.to_excel(\"/home/hb/python/efficientnet_kincnn/model_info.xlsx\", index=False)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 71,
+   "execution_count": 5,
    "metadata": {},
    "outputs": [
     {
@@ -540,6 +2303,8 @@
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
+       "      <th>data</th>\n",
+       "      <th>AUC</th>\n",
        "      <th>conv_stem_kernel_size</th>\n",
        "      <th>conv_stem_stride_size</th>\n",
        "      <th>conv_stem_pooling</th>\n",
@@ -548,187 +2313,1213 @@
        "      <th>MBconv0_oup</th>\n",
        "      <th>MBconv0_kernel_size</th>\n",
        "      <th>MBconv0_stride</th>\n",
-       "      <th>last_features</th>\n",
-       "      <th>AUC</th>\n",
-       "      <th>model_path</th>\n",
+       "      <th>...</th>\n",
+       "      <th>MBconv13_kernel_size</th>\n",
+       "      <th>MBconv13_stride</th>\n",
+       "      <th>MBconv14_inp</th>\n",
+       "      <th>MBconv14_oup</th>\n",
+       "      <th>MBconv14_kernel_size</th>\n",
+       "      <th>MBconv14_stride</th>\n",
+       "      <th>MBconv15_inp</th>\n",
+       "      <th>MBconv15_oup</th>\n",
+       "      <th>MBconv15_kernel_size</th>\n",
+       "      <th>MBconv15_stride</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.8273</td>\n",
        "      <td>(3, 1)</td>\n",
        "      <td>(1, 1)</td>\n",
        "      <td>True</td>\n",
        "      <td>(2, 1)</td>\n",
        "      <td>8</td>\n",
        "      <td>16</td>\n",
-       "      <td>(5, 1)</td>\n",
-       "      <td>(2, 1)</td>\n",
-       "      <td>7920</td>\n",
-       "      <td>0.8262</td>\n",
-       "      <td>DeepPP_kincnn4_1846_bs1024_weight0</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.8202</td>\n",
        "      <td>(3, 1)</td>\n",
        "      <td>(1, 1)</td>\n",
        "      <td>True</td>\n",
        "      <td>(2, 1)</td>\n",
        "      <td>8</td>\n",
        "      <td>16</td>\n",
-       "      <td>(5, 1)</td>\n",
+       "      <td>(5, 5)</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>2</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.8243</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
        "      <td>(2, 1)</td>\n",
-       "      <td>7920</td>\n",
+       "      <td>8</td>\n",
+       "      <td>16</td>\n",
+       "      <td>(5, 3)</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>3</th>\n",
+       "      <td>train1</td>\n",
        "      <td>0.8262</td>\n",
-       "      <td>DeepPP_kincnn4_1846_bs1024_weight0</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>16</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
        "    </tr>\n",
-       "  </tbody>\n",
-       "</table>\n",
-       "</div>"
-      ],
-      "text/plain": [
-       "  conv_stem_kernel_size conv_stem_stride_size  conv_stem_pooling  \\\n",
-       "0                (3, 1)                (1, 1)               True   \n",
-       "1                (3, 1)                (1, 1)               True   \n",
-       "\n",
-       "  conv_stem_pooling_kernel_size  MBconv0_inp  MBconv0_oup MBconv0_kernel_size  \\\n",
-       "0                        (2, 1)            8           16              (5, 1)   \n",
-       "1                        (2, 1)            8           16              (5, 1)   \n",
-       "\n",
-       "  MBconv0_stride  last_features     AUC                          model_path  \n",
-       "0         (2, 1)           7920  0.8262  DeepPP_kincnn4_1846_bs1024_weight0  \n",
-       "1         (2, 1)           7920  0.8262  DeepPP_kincnn4_1846_bs1024_weight0  "
-      ]
-     },
-     "execution_count": 71,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "df"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 55,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "{'conv_stem_kernel_size': (3, 1),\n",
-       " 'conv_stem_stride_size': (1, 1),\n",
-       " 'conv_stem_pooling': True,\n",
-       " 'conv_stem_pooling_kernel_size': (2, 1),\n",
-       " 'MBconv0_inp': 8,\n",
-       " 'MBconv0_oup': 16,\n",
-       " 'MBconv0_kernel_size': (5, 1),\n",
-       " 'MBconv0_stride': (2, 1),\n",
-       " 'last_features': 7920,\n",
-       " 'AUC': 0.8262,\n",
-       " 'model_path': 'DeepPP_kincnn4_1846_bs1024_weight0'}"
-      ]
-     },
-     "execution_count": 55,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "model_params_dict"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 9,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "459d3736b1ca45d29ee819293567bf1b",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "  0%|          | 0/4 [00:00<?, ?it/s]"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "11ec3cd897f94f3ebcfbb35ac2495e34",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "  0%|          | 0/4 [00:00<?, ?it/s]"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "c703b9b5da684bb195bd0c810f513d22",
-       "version_major": 2,
-       "version_minor": 0
-      },
+       "    <tr>\n",
+       "      <th>4</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.6549</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(3, 3)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>(5, 5)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>720.0</td>\n",
+       "      <td>120.0</td>\n",
+       "      <td>(5, 5)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>720.0</td>\n",
+       "      <td>200.0</td>\n",
+       "      <td>(3, 3)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>5</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.8230</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(3, 3)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>6</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.8208</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>7</th>\n",
+       "      <td>train2</td>\n",
+       "      <td>0.8340</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>8</th>\n",
+       "      <td>train3</td>\n",
+       "      <td>0.8339</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>9</th>\n",
+       "      <td>train4</td>\n",
+       "      <td>0.8367</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>10</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.8208</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>11</th>\n",
+       "      <td>train2</td>\n",
+       "      <td>0.8340</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>12</th>\n",
+       "      <td>train3</td>\n",
+       "      <td>0.8339</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>13</th>\n",
+       "      <td>train4</td>\n",
+       "      <td>0.8367</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "<p>14 rows  72 columns</p>\n",
+       "</div>"
+      ],
       "text/plain": [
-       "  0%|          | 0/4 [00:00<?, ?it/s]"
+       "      data     AUC conv_stem_kernel_size conv_stem_stride_size  \\\n",
+       "0   train1  0.8273                (3, 1)                (1, 1)   \n",
+       "1   train1  0.8202                (3, 1)                (1, 1)   \n",
+       "2   train1  0.8243                (3, 1)                (1, 1)   \n",
+       "3   train1  0.8262                (3, 1)                (1, 1)   \n",
+       "4   train1  0.6549                (3, 1)                (1, 1)   \n",
+       "5   train1  0.8230                (3, 1)                (1, 1)   \n",
+       "6   train1  0.8208                (3, 1)                (1, 1)   \n",
+       "7   train2  0.8340                (3, 1)                (1, 1)   \n",
+       "8   train3  0.8339                (3, 1)                (1, 1)   \n",
+       "9   train4  0.8367                (3, 1)                (1, 1)   \n",
+       "10  train1  0.8208                (3, 1)                (1, 1)   \n",
+       "11  train2  0.8340                (3, 1)                (1, 1)   \n",
+       "12  train3  0.8339                (3, 1)                (1, 1)   \n",
+       "13  train4  0.8367                (3, 1)                (1, 1)   \n",
+       "\n",
+       "    conv_stem_pooling conv_stem_pooling_kernel_size  MBconv0_inp  MBconv0_oup  \\\n",
+       "0                True                        (2, 1)            8           16   \n",
+       "1                True                        (2, 1)            8           16   \n",
+       "2                True                        (2, 1)            8           16   \n",
+       "3                True                        (2, 1)            8           16   \n",
+       "4                True                        (2, 1)            8            8   \n",
+       "5                True                        (2, 1)            8            8   \n",
+       "6                True                        (2, 1)            8            8   \n",
+       "7                True                        (2, 1)            8            8   \n",
+       "8                True                        (2, 1)            8            8   \n",
+       "9                True                        (2, 1)            8            8   \n",
+       "10               True                        (2, 1)            8            8   \n",
+       "11               True                        (2, 1)            8            8   \n",
+       "12               True                        (2, 1)            8            8   \n",
+       "13               True                        (2, 1)            8            8   \n",
+       "\n",
+       "   MBconv0_kernel_size MBconv0_stride  ...  MBconv13_kernel_size  \\\n",
+       "0               (3, 1)         (1, 1)  ...                   NaN   \n",
+       "1               (5, 5)         (2, 1)  ...                   NaN   \n",
+       "2               (5, 3)         (2, 1)  ...                   NaN   \n",
+       "3               (5, 1)         (2, 1)  ...                   NaN   \n",
+       "4               (3, 3)         (1, 1)  ...                (5, 5)   \n",
+       "5               (3, 3)         (1, 1)  ...                   NaN   \n",
+       "6               (5, 1)         (1, 1)  ...                   NaN   \n",
+       "7               (5, 1)         (1, 1)  ...                   NaN   \n",
+       "8               (5, 1)         (1, 1)  ...                   NaN   \n",
+       "9               (5, 1)         (1, 1)  ...                   NaN   \n",
+       "10              (5, 1)         (1, 1)  ...                   NaN   \n",
+       "11              (5, 1)         (1, 1)  ...                   NaN   \n",
+       "12              (5, 1)         (1, 1)  ...                   NaN   \n",
+       "13              (5, 1)         (1, 1)  ...                   NaN   \n",
+       "\n",
+       "   MBconv13_stride  MBconv14_inp  MBconv14_oup MBconv14_kernel_size  \\\n",
+       "0              NaN           NaN           NaN                  NaN   \n",
+       "1              NaN           NaN           NaN                  NaN   \n",
+       "2              NaN           NaN           NaN                  NaN   \n",
+       "3              NaN           NaN           NaN                  NaN   \n",
+       "4           (1, 1)         720.0         120.0               (5, 5)   \n",
+       "5              NaN           NaN           NaN                  NaN   \n",
+       "6              NaN           NaN           NaN                  NaN   \n",
+       "7              NaN           NaN           NaN                  NaN   \n",
+       "8              NaN           NaN           NaN                  NaN   \n",
+       "9              NaN           NaN           NaN                  NaN   \n",
+       "10             NaN           NaN           NaN                  NaN   \n",
+       "11             NaN           NaN           NaN                  NaN   \n",
+       "12             NaN           NaN           NaN                  NaN   \n",
+       "13             NaN           NaN           NaN                  NaN   \n",
+       "\n",
+       "   MBconv14_stride  MBconv15_inp  MBconv15_oup MBconv15_kernel_size  \\\n",
+       "0              NaN           NaN           NaN                  NaN   \n",
+       "1              NaN           NaN           NaN                  NaN   \n",
+       "2              NaN           NaN           NaN                  NaN   \n",
+       "3              NaN           NaN           NaN                  NaN   \n",
+       "4           (1, 1)         720.0         200.0               (3, 3)   \n",
+       "5              NaN           NaN           NaN                  NaN   \n",
+       "6              NaN           NaN           NaN                  NaN   \n",
+       "7              NaN           NaN           NaN                  NaN   \n",
+       "8              NaN           NaN           NaN                  NaN   \n",
+       "9              NaN           NaN           NaN                  NaN   \n",
+       "10             NaN           NaN           NaN                  NaN   \n",
+       "11             NaN           NaN           NaN                  NaN   \n",
+       "12             NaN           NaN           NaN                  NaN   \n",
+       "13             NaN           NaN           NaN                  NaN   \n",
+       "\n",
+       "   MBconv15_stride  \n",
+       "0              NaN  \n",
+       "1              NaN  \n",
+       "2              NaN  \n",
+       "3              NaN  \n",
+       "4           (1, 1)  \n",
+       "5              NaN  \n",
+       "6              NaN  \n",
+       "7              NaN  \n",
+       "8              NaN  \n",
+       "9              NaN  \n",
+       "10             NaN  \n",
+       "11             NaN  \n",
+       "12             NaN  \n",
+       "13             NaN  \n",
+       "\n",
+       "[14 rows x 72 columns]"
       ]
      },
+     "execution_count": 5,
      "metadata": {},
-     "output_type": "display_data"
-    },
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "df"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 69,
+   "metadata": {},
+   "outputs": [
     {
      "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "2415a8fbc0034f0c9532355515223a93",
-       "version_major": 2,
-       "version_minor": 0
-      },
       "text/plain": [
-       "  0%|          | 0/4 [00:00<?, ?it/s]"
+       "MBConvBlock(\n",
+       "  (_depthwise_conv): Conv2dStaticSamePadding(\n",
+       "    8, 8, kernel_size=(5, 1), stride=(1, 1), groups=8, bias=False\n",
+       "    (static_padding): ZeroPad2d((0, 0, 2, 2))\n",
+       "  )\n",
+       "  (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "  (_se_reduce): Conv2dStaticSamePadding(\n",
+       "    8, 2, kernel_size=(1, 1), stride=(1, 1)\n",
+       "    (static_padding): Identity()\n",
+       "  )\n",
+       "  (_se_expand): Conv2dStaticSamePadding(\n",
+       "    2, 8, kernel_size=(1, 1), stride=(1, 1)\n",
+       "    (static_padding): Identity()\n",
+       "  )\n",
+       "  (_project_conv): Conv2dStaticSamePadding(\n",
+       "    8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "    (static_padding): Identity()\n",
+       "  )\n",
+       "  (_bn2): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "  (_swish): MemoryEfficientSwish()\n",
+       ")"
       ]
      },
+     "execution_count": 69,
      "metadata": {},
-     "output_type": "display_data"
-    },
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "model[0]._blocks[0]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {},
+   "outputs": [
+    {
+     "ename": "ModuleNotFoundError",
+     "evalue": "No module named 'utils'",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
+      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m efficientnet\n\u001b[1;32m      2\u001b[0m a, b \u001b[39m=\u001b[39m efficientnet()\n",
+      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
+     ]
+    }
+   ],
+   "source": [
+    "from utils import efficientnet\n",
+    "a, b = efficientnet()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 76,
+   "metadata": {},
+   "outputs": [
     {
      "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "b2f6988b8ed44417a686f9e1b9eac159",
-       "version_major": 2,
-       "version_minor": 0
-      },
       "text/plain": [
-       "  0%|          | 0/4 [00:00<?, ?it/s]"
+       "[BlockArgs(num_repeat=1, conv_kernel_size=(5, 1), pool_kernel_size=(3, 1), conv_stride=(1, 1), pool_stride=(3, 1), expand_ratio=1, input_filters=2, output_filters=4, se_ratio=0.25, id_skip=True)]"
       ]
      },
+     "execution_count": 76,
      "metadata": {},
-     "output_type": "display_data"
-    },
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "a"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 77,
+   "metadata": {},
+   "outputs": [
     {
      "data": {
       "text/plain": [
-       "0.8061613750000001"
+       "GlobalParams(width_coefficient=None, depth_coefficient=None, image_size=None, dropout_rate=0.5, num_classes=1, batch_norm_momentum=0.99, batch_norm_epsilon=0.001, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, include_top=None)"
       ]
      },
-     "execution_count": 9,
+     "execution_count": 77,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
-    "df_test = pd.read_pickle(\"/home/hb/python/phospho/data/required/0308_final_train_test/test1.pkl\")\n",
-    "auc = ensemble(model_list_transfer, df_test)\n",
-    "auc"
+    "b"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 72,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "15840\n"
+     ]
+    }
+   ],
+   "source": [
+    "m = kincnn.EfficientNet.from_name('KINCNN')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 73,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "EfficientNet(\n",
+       "  (_conv_stem): Conv2dStaticSamePadding(\n",
+       "    1, 8, kernel_size=(3, 1), stride=(1, 1), bias=False\n",
+       "    (static_padding): ZeroPad2d((0, 0, 1, 1))\n",
+       "  )\n",
+       "  (_bn0): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "  (_max_pooling): MaxPool2dStaticSamePadding(\n",
+       "    kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=[1, 1], ceil_mode=False\n",
+       "    (static_padding): ZeroPad2d((0, 0, 0, 1))\n",
+       "  )\n",
+       "  (_blocks): ModuleList(\n",
+       "    (0): MBConvBlock(\n",
+       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
+       "        8, 8, kernel_size=(5, 1), stride=(1, 1), groups=8, bias=False\n",
+       "        (static_padding): ZeroPad2d((0, 0, 2, 2))\n",
+       "      )\n",
+       "      (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "      (_depthwise_max_pooling): MaxPool2dStaticSamePadding(\n",
+       "        kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=[1, 1], ceil_mode=False\n",
+       "        (static_padding): Identity()\n",
+       "      )\n",
+       "      (_se_reduce): Conv2dStaticSamePadding(\n",
+       "        8, 2, kernel_size=(1, 1), stride=(1, 1)\n",
+       "        (static_padding): Identity()\n",
+       "      )\n",
+       "      (_se_expand): Conv2dStaticSamePadding(\n",
+       "        2, 8, kernel_size=(1, 1), stride=(1, 1)\n",
+       "        (static_padding): Identity()\n",
+       "      )\n",
+       "      (_project_conv): Conv2dStaticSamePadding(\n",
+       "        8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "        (static_padding): Identity()\n",
+       "      )\n",
+       "      (_bn2): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "      (_swish): MemoryEfficientSwish()\n",
+       "    )\n",
+       "  )\n",
+       "  (_conv_head): Conv2dStaticSamePadding(\n",
+       "    8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "    (static_padding): Identity()\n",
+       "  )\n",
+       "  (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "  (_dropout): Dropout(p=0.7, inplace=False)\n",
+       "  (_fc): Linear(in_features=15840, out_features=1, bias=True)\n",
+       "  (_swish): MemoryEfficientSwish()\n",
+       ")"
+      ]
+     },
+     "execution_count": 73,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "m"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 70,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "EfficientNet(\n",
+       "  (_conv_stem): Conv2dStaticSamePadding(\n",
+       "    1, 8, kernel_size=(3, 1), stride=(1, 1), bias=False\n",
+       "    (static_padding): ZeroPad2d((0, 0, 1, 1))\n",
+       "  )\n",
+       "  (_bn0): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "  (_max_pooling): MaxPool2dStaticSamePadding(\n",
+       "    kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=[1, 1], ceil_mode=False\n",
+       "    (static_padding): ZeroPad2d((0, 0, 0, 1))\n",
+       "  )\n",
+       "  (_blocks): ModuleList(\n",
+       "    (0): MBConvBlock(\n",
+       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
+       "        8, 8, kernel_size=(5, 1), stride=(1, 1), groups=8, bias=False\n",
+       "        (static_padding): ZeroPad2d((0, 0, 2, 2))\n",
+       "      )\n",
+       "      (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "      (_se_reduce): Conv2dStaticSamePadding(\n",
+       "        8, 2, kernel_size=(1, 1), stride=(1, 1)\n",
+       "        (static_padding): Identity()\n",
+       "      )\n",
+       "      (_se_expand): Conv2dStaticSamePadding(\n",
+       "        2, 8, kernel_size=(1, 1), stride=(1, 1)\n",
+       "        (static_padding): Identity()\n",
+       "      )\n",
+       "      (_project_conv): Conv2dStaticSamePadding(\n",
+       "        8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "        (static_padding): Identity()\n",
+       "      )\n",
+       "      (_bn2): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "      (_swish): MemoryEfficientSwish()\n",
+       "    )\n",
+       "  )\n",
+       "  (_conv_head): Conv2dStaticSamePadding(\n",
+       "    8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
+       "    (static_padding): Identity()\n",
+       "  )\n",
+       "  (_bn1): BatchNorm2d(8, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
+       "  (_dropout): Dropout(p=0.7, inplace=False)\n",
+       "  (_fc): Linear(in_features=15840, out_features=1, bias=True)\n",
+       "  (_swish): MemoryEfficientSwish()\n",
+       ")"
+      ]
+     },
+     "execution_count": 70,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "model[0]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 61,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>data</th>\n",
+       "      <th>AUC</th>\n",
+       "      <th>conv_stem_kernel_size</th>\n",
+       "      <th>conv_stem_stride_size</th>\n",
+       "      <th>conv_stem_pooling</th>\n",
+       "      <th>conv_stem_pooling_kernel_size</th>\n",
+       "      <th>MBconv0_inp</th>\n",
+       "      <th>MBconv0_oup</th>\n",
+       "      <th>MBconv0_kernel_size</th>\n",
+       "      <th>MBconv0_stride</th>\n",
+       "      <th>...</th>\n",
+       "      <th>MBconv13_kernel_size</th>\n",
+       "      <th>MBconv13_stride</th>\n",
+       "      <th>MBconv14_inp</th>\n",
+       "      <th>MBconv14_oup</th>\n",
+       "      <th>MBconv14_kernel_size</th>\n",
+       "      <th>MBconv14_stride</th>\n",
+       "      <th>MBconv15_inp</th>\n",
+       "      <th>MBconv15_oup</th>\n",
+       "      <th>MBconv15_kernel_size</th>\n",
+       "      <th>MBconv15_stride</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <th>0</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.8273</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>16</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>1</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.8202</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>16</td>\n",
+       "      <td>(5, 5)</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>2</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.8243</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>16</td>\n",
+       "      <td>(5, 3)</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>3</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.8262</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>16</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>4</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.6549</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(3, 3)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>(5, 5)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>720.0</td>\n",
+       "      <td>120.0</td>\n",
+       "      <td>(5, 5)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>720.0</td>\n",
+       "      <td>200.0</td>\n",
+       "      <td>(3, 3)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>5</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.8230</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(3, 3)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>6</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.8208</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>7</th>\n",
+       "      <td>train2</td>\n",
+       "      <td>0.8340</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>8</th>\n",
+       "      <td>train3</td>\n",
+       "      <td>0.8339</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>9</th>\n",
+       "      <td>train4</td>\n",
+       "      <td>0.8367</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>10</th>\n",
+       "      <td>train1</td>\n",
+       "      <td>0.8208</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>11</th>\n",
+       "      <td>train2</td>\n",
+       "      <td>0.8340</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>12</th>\n",
+       "      <td>train3</td>\n",
+       "      <td>0.8339</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <th>13</th>\n",
+       "      <td>train4</td>\n",
+       "      <td>0.8367</td>\n",
+       "      <td>(3, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>True</td>\n",
+       "      <td>(2, 1)</td>\n",
+       "      <td>8</td>\n",
+       "      <td>8</td>\n",
+       "      <td>(5, 1)</td>\n",
+       "      <td>(1, 1)</td>\n",
+       "      <td>...</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "      <td>NaN</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "<p>14 rows  72 columns</p>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "      data     AUC conv_stem_kernel_size conv_stem_stride_size  \\\n",
+       "0   train1  0.8273                (3, 1)                (1, 1)   \n",
+       "1   train1  0.8202                (3, 1)                (1, 1)   \n",
+       "2   train1  0.8243                (3, 1)                (1, 1)   \n",
+       "3   train1  0.8262                (3, 1)                (1, 1)   \n",
+       "4   train1  0.6549                (3, 1)                (1, 1)   \n",
+       "5   train1  0.8230                (3, 1)                (1, 1)   \n",
+       "6   train1  0.8208                (3, 1)                (1, 1)   \n",
+       "7   train2  0.8340                (3, 1)                (1, 1)   \n",
+       "8   train3  0.8339                (3, 1)                (1, 1)   \n",
+       "9   train4  0.8367                (3, 1)                (1, 1)   \n",
+       "10  train1  0.8208                (3, 1)                (1, 1)   \n",
+       "11  train2  0.8340                (3, 1)                (1, 1)   \n",
+       "12  train3  0.8339                (3, 1)                (1, 1)   \n",
+       "13  train4  0.8367                (3, 1)                (1, 1)   \n",
+       "\n",
+       "    conv_stem_pooling conv_stem_pooling_kernel_size  MBconv0_inp  MBconv0_oup  \\\n",
+       "0                True                        (2, 1)            8           16   \n",
+       "1                True                        (2, 1)            8           16   \n",
+       "2                True                        (2, 1)            8           16   \n",
+       "3                True                        (2, 1)            8           16   \n",
+       "4                True                        (2, 1)            8            8   \n",
+       "5                True                        (2, 1)            8            8   \n",
+       "6                True                        (2, 1)            8            8   \n",
+       "7                True                        (2, 1)            8            8   \n",
+       "8                True                        (2, 1)            8            8   \n",
+       "9                True                        (2, 1)            8            8   \n",
+       "10               True                        (2, 1)            8            8   \n",
+       "11               True                        (2, 1)            8            8   \n",
+       "12               True                        (2, 1)            8            8   \n",
+       "13               True                        (2, 1)            8            8   \n",
+       "\n",
+       "   MBconv0_kernel_size MBconv0_stride  ...  MBconv13_kernel_size  \\\n",
+       "0               (3, 1)         (1, 1)  ...                   NaN   \n",
+       "1               (5, 5)         (2, 1)  ...                   NaN   \n",
+       "2               (5, 3)         (2, 1)  ...                   NaN   \n",
+       "3               (5, 1)         (2, 1)  ...                   NaN   \n",
+       "4               (3, 3)         (1, 1)  ...                (5, 5)   \n",
+       "5               (3, 3)         (1, 1)  ...                   NaN   \n",
+       "6               (5, 1)         (1, 1)  ...                   NaN   \n",
+       "7               (5, 1)         (1, 1)  ...                   NaN   \n",
+       "8               (5, 1)         (1, 1)  ...                   NaN   \n",
+       "9               (5, 1)         (1, 1)  ...                   NaN   \n",
+       "10              (5, 1)         (1, 1)  ...                   NaN   \n",
+       "11              (5, 1)         (1, 1)  ...                   NaN   \n",
+       "12              (5, 1)         (1, 1)  ...                   NaN   \n",
+       "13              (5, 1)         (1, 1)  ...                   NaN   \n",
+       "\n",
+       "   MBconv13_stride  MBconv14_inp  MBconv14_oup MBconv14_kernel_size  \\\n",
+       "0              NaN           NaN           NaN                  NaN   \n",
+       "1              NaN           NaN           NaN                  NaN   \n",
+       "2              NaN           NaN           NaN                  NaN   \n",
+       "3              NaN           NaN           NaN                  NaN   \n",
+       "4           (1, 1)         720.0         120.0               (5, 5)   \n",
+       "5              NaN           NaN           NaN                  NaN   \n",
+       "6              NaN           NaN           NaN                  NaN   \n",
+       "7              NaN           NaN           NaN                  NaN   \n",
+       "8              NaN           NaN           NaN                  NaN   \n",
+       "9              NaN           NaN           NaN                  NaN   \n",
+       "10             NaN           NaN           NaN                  NaN   \n",
+       "11             NaN           NaN           NaN                  NaN   \n",
+       "12             NaN           NaN           NaN                  NaN   \n",
+       "13             NaN           NaN           NaN                  NaN   \n",
+       "\n",
+       "   MBconv14_stride  MBconv15_inp  MBconv15_oup MBconv15_kernel_size  \\\n",
+       "0              NaN           NaN           NaN                  NaN   \n",
+       "1              NaN           NaN           NaN                  NaN   \n",
+       "2              NaN           NaN           NaN                  NaN   \n",
+       "3              NaN           NaN           NaN                  NaN   \n",
+       "4           (1, 1)         720.0         200.0               (3, 3)   \n",
+       "5              NaN           NaN           NaN                  NaN   \n",
+       "6              NaN           NaN           NaN                  NaN   \n",
+       "7              NaN           NaN           NaN                  NaN   \n",
+       "8              NaN           NaN           NaN                  NaN   \n",
+       "9              NaN           NaN           NaN                  NaN   \n",
+       "10             NaN           NaN           NaN                  NaN   \n",
+       "11             NaN           NaN           NaN                  NaN   \n",
+       "12             NaN           NaN           NaN                  NaN   \n",
+       "13             NaN           NaN           NaN                  NaN   \n",
+       "\n",
+       "   MBconv15_stride  \n",
+       "0              NaN  \n",
+       "1              NaN  \n",
+       "2              NaN  \n",
+       "3              NaN  \n",
+       "4           (1, 1)  \n",
+       "5              NaN  \n",
+       "6              NaN  \n",
+       "7              NaN  \n",
+       "8              NaN  \n",
+       "9              NaN  \n",
+       "10             NaN  \n",
+       "11             NaN  \n",
+       "12             NaN  \n",
+       "13             NaN  \n",
+       "\n",
+       "[14 rows x 72 columns]"
+      ]
+     },
+     "execution_count": 61,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "df"
    ]
   },
   {
diff --git a/code/phospho_preprocessing.py b/code/phospho_preprocessing.py
index 9a9621b..9cee016 100644
--- a/code/phospho_preprocessing.py
+++ b/code/phospho_preprocessing.py
@@ -16,22 +16,22 @@ import pickle
 
 def prepare_dataset(dataset_mode):
     
-    # if dataset_mode == 'transfer_learning_1':
+    if dataset_mode == 'kincnn1':
         
-    df = pd.read_pickle("/home/hb/python/phospho/data/required/0308_final_train_test/train1.pkl")
-    print(df.iloc[-1:])
-    # elif dataset_mode == 'transfer_learning_2':
+        df = pd.read_pickle("/home/hb/python/phospho/data/required/0308_final_train_test/train1.pkl")
+        print(df.iloc[-1:])
+    elif dataset_mode == 'kincnn2':
         
-    #     df = pd.read_pickle("/home/hb/python/phospho/data/required/0308_final_train_test/train2.pkl")
-    #     print(df.iloc[-1:])
-    # elif dataset_mode == 'transfer_learning_3':
+        df = pd.read_pickle("/home/hb/python/phospho/data/required/0308_final_train_test/train2.pkl")
+        print(df.iloc[-1:])
+    elif dataset_mode == 'kincnn3':
         
-    #     df = pd.read_pickle("/home/hb/python/phospho/data/required/0308_final_train_test/train3.pkl")
-    #     print(df.iloc[-1:])
-    # elif dataset_mode == 'transfer_learning_4':
+        df = pd.read_pickle("/home/hb/python/phospho/data/required/0308_final_train_test/train3.pkl")
+        print(df.iloc[-1:])
+    elif dataset_mode == 'kincnn4':
         
-    #     df = pd.read_pickle("/home/hb/python/phospho/data/required/0308_final_train_test/train4.pkl")
-    #     print(df.iloc[-1:])
+        df = pd.read_pickle("/home/hb/python/phospho/data/required/0308_final_train_test/train4.pkl")
+        print(df.iloc[-1:])
     # elif dataset_mode == 'transfer_learning_5':
         
     #     df = pd.read_pickle("/home/hb/python/phospho/data/required/0308_final_train_test/train5.pkl")
diff --git a/gpu_1.sh b/gpu_1.sh
index e665121..17d2aab 100755
--- a/gpu_1.sh
+++ b/gpu_1.sh
@@ -25,6 +25,6 @@
 #     config.pretrain_fold_num = sys.argv[10]
 #     config.model = f'efficientnet-phospho-B-15'
 #     config.save_dir = f'/home/hb/python/efficientnet_kincnn/saved_model/{datetime.today().strftime("%m%d")}/DeepPP_{config.dataset}_{datetime.today().strftime("%H%M")}_bs{config.batch_size}_weight{config.pretrain_fold_num}'
-python3 /home/hb/python/efficientnet_kincnn/DeepPhospho2.py 1 1024 500 1e-3 5 7 0.7 50 kincnn2 0
+python3 /home/hb/python/efficientnet_kincnn/DeepPhospho2.py 2 512 500 1e-3 5 7 0.7 50 kincnn2 0
 
 
diff --git a/kincnn.py b/kincnn.py
index d07913a..eb8e344 100644
--- a/kincnn.py
+++ b/kincnn.py
@@ -119,7 +119,6 @@ class MBConvBlock(nn.Module):
             x = self._swish(x)
 
         x = self._depthwise_conv(x)  # 2
-        # x = partial()
         x = self._bn1(x)
         if self._block_args.pool_kernel_size:
             x = self._depthwise_max_pooling(x)
@@ -233,7 +232,7 @@ class EfficientNet(nn.Module):
             image_size = calculate_output_image_size(image_size, block_args.conv_stride)
             if block_args.num_repeat > 1:  # modify block_args to keep same output size
                 block_args = block_args._replace(
-                    input_filters=block_args.output_filters, stride=1
+                    input_filters=block_args.output_filters, conv_stride=(1, 1)
                 )
             for _ in range(block_args.num_repeat - 1):
                 self._blocks.append(
@@ -254,6 +253,7 @@ class EfficientNet(nn.Module):
 
         # Final linear layer
         # self._avg_pooling = nn.AdaptiveAvgPool2d(1)  # <-- 
+        out_features = image_size[0] * image_size[1] * out_channels
         self._dropout = nn.Dropout(self._global_params.dropout_rate)
         self._fc = nn.Linear(out_features, self._global_params.num_classes)
 
diff --git a/kincnn2.py b/kincnn2.py
index 9afb8a1..3796947 100644
--- a/kincnn2.py
+++ b/kincnn2.py
@@ -232,7 +232,7 @@ class EfficientNet(nn.Module):
             image_size = calculate_output_image_size(image_size, block_args.conv_stride)
             if block_args.num_repeat > 1:  # modify block_args to keep same output size
                 block_args = block_args._replace(
-                    input_filters=block_args.output_filters, stride=1
+                    input_filters=block_args.output_filters, conv_stride=(1, 1)
                 )
             for _ in range(block_args.num_repeat - 1):
                 self._blocks.append(
diff --git a/test.ipynb b/test.ipynb
index 41c18b9..49d2b72 100644
--- a/test.ipynb
+++ b/test.ipynb
@@ -2,6 +2,7 @@
  "cells": [
   {
    "cell_type": "code",
+<<<<<<< HEAD
    "execution_count": 3,
    "metadata": {},
    "outputs": [],
@@ -178,12 +179,12 @@
   },
   {
    "cell_type": "code",
+=======
+>>>>>>> 7d235b7e18d2f60ff0dc8e254a1387c542004c24
    "execution_count": 1,
-   "metadata": {
-    "collapsed": true
-   },
    "outputs": [],
    "source": [
+<<<<<<< HEAD
     "import re\n",
     "import collections\n",
     "import torch\n",
@@ -268,15 +269,22 @@
     "\n",
     "    return blocks_args, global_params\n",
     "\n",
+=======
+    "from utils import efficientnet\n",
+>>>>>>> 7d235b7e18d2f60ff0dc8e254a1387c542004c24
     "blocks_args, global_params = efficientnet(width_coefficient=1.0, depth_coefficient=1.0, dropout_rate=0.7, image_size=[263, 15])"
    ]
   },
   {
    "cell_type": "code",
+<<<<<<< HEAD
    "execution_count": 227,
    "metadata": {
     "collapsed": false
    },
+=======
+   "execution_count": 2,
+>>>>>>> 7d235b7e18d2f60ff0dc8e254a1387c542004c24
    "outputs": [
     {
      "name": "stdout",
@@ -284,11 +292,7 @@
      "text": [
       "GlobalParams(width_coefficient=1.0, depth_coefficient=1.0, image_size=[263, 15], dropout_rate=0.7, num_classes=1, batch_norm_momentum=0.99, batch_norm_epsilon=0.001, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, include_top=None)\n",
       "\n",
-      "MBConvblock 0:  BlockArgs(num_repeat=1, kernel_size=(8, 2), stride=(2, 2), expand_ratio=2, input_filters=8, output_filters=16, se_ratio=0.25, id_skip=True)\n",
-      "\n",
-      "MBConvblock 1:  BlockArgs(num_repeat=1, kernel_size=(8, 3), stride=(2, 1), expand_ratio=2, input_filters=16, output_filters=32, se_ratio=0.25, id_skip=True)\n",
-      "\n",
-      "MBConvblock 2:  BlockArgs(num_repeat=1, kernel_size=(6, 3), stride=(2, 2), expand_ratio=2, input_filters=32, output_filters=64, se_ratio=0.25, id_skip=True)\n",
+      "MBConvblock 0:  BlockArgs(num_repeat=1, conv_kernel_size=(5, 1), pool_kernel_size=(3, 1), conv_stride=(1, 1), pool_stride=(3, 1), expand_ratio=1, input_filters=2, output_filters=4, se_ratio=0.25, id_skip=True)\n",
       "\n"
      ]
     }
@@ -303,6 +307,7 @@
   },
   {
    "cell_type": "code",
+<<<<<<< HEAD
    "execution_count": 228,
    "metadata": {
     "collapsed": false
@@ -336,11 +341,27 @@
    "metadata": {
     "collapsed": false
    },
+=======
+   "execution_count": null,
+   "outputs": [],
+   "source": [],
+   "metadata": {
+    "collapsed": false
+   }
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 12,
+>>>>>>> 7d235b7e18d2f60ff0dc8e254a1387c542004c24
    "outputs": [],
    "source": [
+    "from utils import Conv2dStaticSamePadding, MaxPool2dStaticSamePadding\n",
+    "from functools import partial\n",
+    "\n",
     "bn_mom = 1 - global_params.batch_norm_momentum\n",
     "bn_eps = global_params.batch_norm_epsilon\n",
     "image_size = global_params.image_size\n",
+<<<<<<< HEAD
     "Conv2d = partial(Conv2dStaticSamePadding, image_size=image_size)"
    ]
   },
@@ -356,19 +377,39 @@
       "text/plain": [
        "[132, 15]"
       ]
+=======
+    "Conv2d = partial(Conv2dStaticSamePadding, image_size=image_size)\n",
+    "MaxPool2d = partial(MaxPool2dStaticSamePadding_, image_size=image_size)"
+   ],
+   "metadata": {
+    "collapsed": false
+   }
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 13,
+   "outputs": [
+    {
+     "data": {
+      "text/plain": "[263, 15]"
+>>>>>>> 7d235b7e18d2f60ff0dc8e254a1387c542004c24
      },
-     "execution_count": 237,
+     "execution_count": 13,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
+    "from torch import nn\n",
+    "from utils import round_filters, calculate_output_image_size, MemoryEfficientSwish\n",
+    "\n",
     "# Stem\n",
     "image_size = [263, 15]\n",
     "in_channels = 1\n",
     "out_channels = round_filters(8, global_params)\n",
-    "_conv_stem = Conv2d(in_channels, out_channels, kernel_size=(15, 1), stride=(2, 1), bias=False, image_size=image_size)\n",
+    "_conv_stem = Conv2d(in_channels, out_channels, kernel_size=(5, 1), stride=(1, 1), bias=False, image_size=image_size)\n",
     "_bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
+<<<<<<< HEAD
     "image_size = calculate_output_image_size_(image_size, stride=(2, 1))\n",
     "image_size"
    ]
@@ -965,11 +1006,17 @@
     }
    ],
    "source": [
+=======
+    "_max_pool = MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
+    "_swish = MemoryEfficientSwish()\n",
+    "image_size = calculate_output_image_size(image_size, stride=(1, 1))\n",
+>>>>>>> 7d235b7e18d2f60ff0dc8e254a1387c542004c24
     "image_size"
    ]
   },
   {
    "cell_type": "code",
+<<<<<<< HEAD
    "execution_count": 119,
    "metadata": {
     "collapsed": false
@@ -1012,13 +1059,45 @@
       "text/plain": [
        "torch.Size([1, 8, 132, 15])"
       ]
+=======
+   "execution_count": 1,
+   "outputs": [],
+   "source": [
+    "import kincnn\n",
+    "import torch"
+   ],
+   "metadata": {
+    "collapsed": false
+   }
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "outputs": [],
+   "source": [
+    "x = torch.randn(1, 1, 263, 15)\n",
+    "model = kincnn.EfficientNet.from_name('efficientnet-phospho-B-15')"
+   ],
+   "metadata": {
+    "collapsed": false
+   }
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "outputs": [
+    {
+     "data": {
+      "text/plain": "tensor([[0.6308]], grad_fn=<SigmoidBackward0>)"
+>>>>>>> 7d235b7e18d2f60ff0dc8e254a1387c542004c24
      },
-     "execution_count": 121,
+     "execution_count": 3,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
+<<<<<<< HEAD
     "# Stem\n",
     "x0 = torch.randn((1, 1, 263, 15))\n",
     "x1 = _conv_stem(x0)\n",
@@ -1044,22 +1123,80 @@
    "metadata": {
     "collapsed": false
    },
+=======
+    "model(x)"
+   ],
+   "metadata": {
+    "collapsed": false
+   }
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 7,
+   "outputs": [],
+   "source": [
+    "import pytorch_model_summary"
+   ],
+   "metadata": {
+    "collapsed": false
+   }
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
+>>>>>>> 7d235b7e18d2f60ff0dc8e254a1387c542004c24
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "3\n",
-      "torch.Size([1, 8, 132, 15])\n",
-      "torch.Size([1, 16, 66, 8])\n",
-      "torch.Size([1, 16, 66, 8])\n",
-      "torch.Size([1, 32, 33, 8])\n",
-      "torch.Size([1, 32, 33, 8])\n",
-      "torch.Size([1, 64, 17, 4])\n"
+      "-------------------------------------------------------------------------------------\n",
+      "                    Layer (type)         Input Shape         Param #     Tr. Param #\n",
+      "=====================================================================================\n",
+      "                     ZeroPad2d-1     [1, 1, 263, 15]               0               0\n",
+      "                   BatchNorm2d-2     [1, 8, 263, 15]              16              16\n",
+      "                     ZeroPad2d-3     [1, 8, 263, 15]               0               0\n",
+      "          MemoryEfficientSwish-4     [1, 8, 132, 15]               0               0\n",
+      "       Conv2dStaticSamePadding-5     [1, 8, 132, 15]              40              40\n",
+      "                   BatchNorm2d-6     [1, 8, 132, 15]              16              16\n",
+      "    MaxPool2dStaticSamePadding-7     [1, 8, 132, 15]               0               0\n",
+      "          MemoryEfficientSwish-8      [1, 8, 44, 15]               0               0\n",
+      "       Conv2dStaticSamePadding-9        [1, 8, 1, 1]              18              18\n",
+      "      Conv2dStaticSamePadding-10        [1, 2, 1, 1]              24              24\n",
+      "      Conv2dStaticSamePadding-11      [1, 8, 44, 15]              64              64\n",
+      "                  BatchNorm2d-12      [1, 8, 44, 15]              16              16\n",
+      "      Conv2dStaticSamePadding-13      [1, 8, 44, 15]              24              24\n",
+      "                  BatchNorm2d-14      [1, 8, 44, 15]              16              16\n",
+      "   MaxPool2dStaticSamePadding-15      [1, 8, 44, 15]               0               0\n",
+      "         MemoryEfficientSwish-16      [1, 8, 22, 15]               0               0\n",
+      "      Conv2dStaticSamePadding-17        [1, 8, 1, 1]              18              18\n",
+      "      Conv2dStaticSamePadding-18        [1, 2, 1, 1]              24              24\n",
+      "      Conv2dStaticSamePadding-19      [1, 8, 22, 15]             128             128\n",
+      "                  BatchNorm2d-20     [1, 16, 22, 15]              32              32\n",
+      "      Conv2dStaticSamePadding-21     [1, 16, 22, 15]             144             144\n",
+      "                  BatchNorm2d-22     [1, 16, 22, 15]              32              32\n",
+      "   MaxPool2dStaticSamePadding-23     [1, 16, 22, 15]               0               0\n",
+      "         MemoryEfficientSwish-24      [1, 16, 11, 8]               0               0\n",
+      "      Conv2dStaticSamePadding-25       [1, 16, 1, 1]              68              68\n",
+      "      Conv2dStaticSamePadding-26        [1, 4, 1, 1]              80              80\n",
+      "      Conv2dStaticSamePadding-27      [1, 16, 11, 8]             512             512\n",
+      "                  BatchNorm2d-28      [1, 32, 11, 8]              64              64\n",
+      "                     Identity-29      [1, 32, 11, 8]               0               0\n",
+      "                  BatchNorm2d-30       [1, 8, 11, 8]              16              16\n",
+      "            AdaptiveAvgPool2d-31       [1, 8, 11, 8]               0               0\n",
+      "                      Dropout-32              [1, 8]               0               0\n",
+      "                       Linear-33              [1, 8]               9               9\n",
+      "=====================================================================================\n",
+      "Total params: 1,361\n",
+      "Trainable params: 1,361\n",
+      "Non-trainable params: 0\n",
+      "Batch size: 1\n",
+      "-------------------------------------------------------------------------------------\n"
      ]
     }
    ],
    "source": [
+<<<<<<< HEAD
     "print(len(blocks))\n",
     "\n",
     "for idx, block in enumerate(blocks):\n",
@@ -1144,13 +1281,75 @@
       "text/plain": [
        "torch.Size([1, 128, 17, 4])"
       ]
+=======
+    "print(pytorch_model_summary.summary(model, torch.zeros(1, 1, 263, 15), show_input=True, show_hierarchical=False, max_depth=2, batch_size=1))"
+   ],
+   "metadata": {
+    "collapsed": false
+   }
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 15,
+   "outputs": [
+    {
+     "ename": "RuntimeError",
+     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [2, 1, 1, 267, 15]",
+     "output_type": "error",
+     "traceback": [
+      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
+      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
+      "Cell \u001B[0;32mIn[15], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchsummary\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m summary\n\u001B[0;32m----> 2\u001B[0m \u001B[43msummary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m263\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
+      "File \u001B[0;32m/opt/anaconda3/envs/efficientnet/lib/python3.8/site-packages/torchsummary/torchsummary.py:72\u001B[0m, in \u001B[0;36msummary\u001B[0;34m(model, input_size, batch_size, device)\u001B[0m\n\u001B[1;32m     68\u001B[0m model\u001B[38;5;241m.\u001B[39mapply(register_hook)\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# make a forward pass\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m# print(x.shape)\u001B[39;00m\n\u001B[0;32m---> 72\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# remove these hooks\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m hooks:\n",
+      "File \u001B[0;32m/opt/anaconda3/envs/efficientnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1500\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1502\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
+      "File \u001B[0;32m/opt/anaconda3/envs/efficientnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1506\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1507\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1509\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1510\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1512\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
+      "File \u001B[0;32m/Users/hb/Dropbox/github/efficientnet_kincnn/kincnn.py:344\u001B[0m, in \u001B[0;36mEfficientNet.forward\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    340\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Calls extract_features to extract features, applies final linear layer, and returns logits.\"\"\"\u001B[39;00m\n\u001B[1;32m    341\u001B[0m \u001B[38;5;66;03m# bs = inputs.size(0)\u001B[39;00m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;66;03m# print(bs)\u001B[39;00m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;66;03m# Convolution layers\u001B[39;00m\n\u001B[0;32m--> 344\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# Pooling and final linear layer\u001B[39;00m\n\u001B[1;32m    347\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_avg_pooling(x)\n",
+      "File \u001B[0;32m/Users/hb/Dropbox/github/efficientnet_kincnn/kincnn.py:320\u001B[0m, in \u001B[0;36mEfficientNet.extract_features\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns output of the final convolution layer\"\"\"\u001B[39;00m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;66;03m# Stem\u001B[39;00m\n\u001B[0;32m--> 320\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_stem\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bn0(x)\n\u001B[1;32m    322\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_pooling(x)\n",
+      "File \u001B[0;32m/opt/anaconda3/envs/efficientnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1500\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1502\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
+      "File \u001B[0;32m/opt/anaconda3/envs/efficientnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1548\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1545\u001B[0m     bw_hook \u001B[38;5;241m=\u001B[39m hooks\u001B[38;5;241m.\u001B[39mBackwardHook(\u001B[38;5;28mself\u001B[39m, full_backward_hooks, backward_pre_hooks)\n\u001B[1;32m   1546\u001B[0m     args \u001B[38;5;241m=\u001B[39m bw_hook\u001B[38;5;241m.\u001B[39msetup_input_hook(args)\n\u001B[0;32m-> 1548\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1549\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n\u001B[1;32m   1550\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m (\n\u001B[1;32m   1551\u001B[0m         \u001B[38;5;241m*\u001B[39m_global_forward_hooks\u001B[38;5;241m.\u001B[39mitems(),\n\u001B[1;32m   1552\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mitems(),\n\u001B[1;32m   1553\u001B[0m     ):\n",
+      "File \u001B[0;32m/Users/hb/Dropbox/github/efficientnet_kincnn/utils.py:338\u001B[0m, in \u001B[0;36mConv2dStaticSamePadding.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    336\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m    337\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstatic_padding(x)\n\u001B[0;32m--> 338\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    339\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
+      "\u001B[0;31mRuntimeError\u001B[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [2, 1, 1, 267, 15]"
+     ]
+    }
+   ],
+   "source": [
+    "from torchsummary import summary\n",
+    "summary(model, input_size=(1, 1, 263, 15))"
+   ],
+   "metadata": {
+    "collapsed": false
+   }
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 20,
+   "outputs": [],
+   "source": [
+    "from utils import round_filters\n",
+    "out_channels = round_filters(\n",
+    "            2, global_params\n",
+    "        )  # number of output channels"
+   ],
+   "metadata": {
+    "collapsed": false
+   }
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 21,
+   "outputs": [
+    {
+     "data": {
+      "text/plain": "GlobalParams(width_coefficient=1.0, depth_coefficient=1.0, image_size=[263, 15], dropout_rate=0.7, num_classes=1, batch_norm_momentum=0.99, batch_norm_epsilon=0.001, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, include_top=None)"
+>>>>>>> 7d235b7e18d2f60ff0dc8e254a1387c542004c24
      },
-     "execution_count": 273,
+     "execution_count": 21,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
+<<<<<<< HEAD
     "x = _conv_head(x)\n",
     "x = _bn1(x)\n",
     "x = _swish(x)\n",
@@ -1180,8 +1379,23 @@
       "text/plain": [
        "128"
       ]
+=======
+    "global_params"
+   ],
+   "metadata": {
+    "collapsed": false
+   }
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 22,
+   "outputs": [
+    {
+     "data": {
+      "text/plain": "8"
+>>>>>>> 7d235b7e18d2f60ff0dc8e254a1387c542004c24
      },
-     "execution_count": 295,
+     "execution_count": 22,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -1192,6 +1406,7 @@
   },
   {
    "cell_type": "code",
+<<<<<<< HEAD
    "execution_count": 292,
    "metadata": {
     "collapsed": false
@@ -1643,6 +1858,8 @@
   },
   {
    "cell_type": "code",
+=======
+>>>>>>> 7d235b7e18d2f60ff0dc8e254a1387c542004c24
    "execution_count": null,
    "metadata": {},
    "outputs": [],
diff --git a/train.py b/train.py
new file mode 100644
index 0000000..4a684f8
--- /dev/null
+++ b/train.py
@@ -0,0 +1,340 @@
+import sys
+
+# sys.path.append('/home/hb/anaconda3/envs/pp_predict/lib/python3.9/site-packages')
+# # sys.path.append('/home/hb/python/neoantigen/from_ju/IEDB_data_filtering')
+# # # sys.path.append('/home/hb/python/neoantigen/code/module')
+sys.path.append("/home/hb/python/phospho/code/module")
+import os
+import random
+from datetime import datetime
+
+import numpy as np
+import torch
+import torch.utils.data as data_utils
+import wandb
+from EarlyStopping import EarlyStopping
+from matplotlib import pyplot as plt
+from module.efficientnet_new import EfficientNet
+from phospho_preprocessing import AttrDict, prepare_dataset
+from precision_recall import precision_recall
+from Radam import RAdam
+from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
+from sklearn.model_selection import KFold, StratifiedShuffleSplit
+from torch import nn
+from torch.autograd import Variable
+from torch.utils.data import ConcatDataset, DataLoader, Dataset
+from tqdm.auto import tqdm
+
+if __name__ == "__main__":
+    config = AttrDict()
+    config.gpu_num = sys.argv[1]
+    config.batch_size = int(sys.argv[2])
+    config.n_epoch = int(sys.argv[3])
+    config.defalut_learning_rate = float(sys.argv[4])
+    config.fold_num = int(sys.argv[5])
+    config.scheduler_patience, config.scheduler_factor = int(sys.argv[6]), float(
+        sys.argv[7]
+    )
+    config.erls_patience = int(sys.argv[8])
+    config.dataset = sys.argv[9]
+    config.pretrain_fold_num = sys.argv[10]
+    config.model = f"efficientnet-phospho-B-15"
+    config.save_dir = f'/home/hb/python/phospho/saved_model/{datetime.today().strftime("%m%d")}/DeepPP_{config.dataset}_{datetime.today().strftime("%H%M")}_bs{config.batch_size}_weight{config.pretrain_fold_num}'
+
+    os.makedirs(f"{config.save_dir}", exist_ok=True)
+
+    import yaml
+
+    with open(f"{config.save_dir}/config.yaml", "w") as f:
+        yaml.dump(config, f)
+
+
+def seed_everything(seed: int = 42):
+    random.seed(seed)
+    np.random.seed(seed)
+    os.environ["PYTHONHASHSEED"] = str(seed)
+    torch.manual_seed(seed)
+    torch.cuda.manual_seed(seed)  # type: ignore
+    torch.backends.cudnn.deterministic = True  # type: ignore
+    torch.backends.cudnn.benchmark = False  # type: ignore
+
+
+seed_everything(42)
+os.environ["CUDA_VISIBLE_DEVICES"] = config.gpu_num  # Set the GPU number to use
+device = torch.device(f"cuda:{config.gpu_num}" if torch.cuda.is_available() else "cpu")
+
+print("Device:", device)
+print("Current cuda device:", torch.cuda.current_device())
+print(f"Using CUDA_VISIBLE_DEVICES {config.gpu_num}")
+print("Count of using GPUs:", torch.cuda.device_count())
+
+"""prepare dataset"""
+dtset = prepare_dataset(dataset_mode=config.dataset)
+train_set = dtset[0]
+valid_set = dtset[1]
+print(train_set, valid_set)
+train_loader = data_utils.DataLoader(
+    train_set, batch_size=config.batch_size, pin_memory=True, shuffle=True
+)
+valid_loader = data_utils.DataLoader(
+    valid_set,
+    batch_size=config.batch_size,
+)
+
+dataloaders = {"train": train_loader, "valid": valid_loader}
+dataset_sizes = {x: len(dataloaders[x]) for x in ["train", "valid"]}
+dataset = ConcatDataset([train_set, valid_set])
+
+
+def train_model_5cv():
+    # dataset = data_utils.DataLoader(, batch_size=config.batch_size, pin_memory=True, shuffle=True)
+    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+    kfold = KFold(n_splits=5, shuffle=True)
+
+    wandb.init(project="phospho", entity="jeguring", reinit=True, config=config)
+    print(config)
+    project_name = f'bs{config.batch_size}_{datetime.today().strftime("%m%d%H%M")}'
+    wandb.run.name = project_name
+
+    for fold, (train_idx, valid_idx) in enumerate(kfold.split(dataset)):
+        globals()[f"{fold}_train_loss"] = []
+        globals()[f"{fold}_train_precision"] = []
+        globals()[f"{fold}_train_recall"] = []
+        globals()[f"{fold}_train_f1"] = []
+        globals()[f"{fold}_train_acc"] = []
+
+        globals()[f"{fold}_valid_loss"] = []
+        globals()[f"{fold}_valid_precision"] = []
+        globals()[f"{fold}_valid_recall"] = []
+        globals()[f"{fold}_valid_f1"] = []
+        globals()[f"{fold}_valid_acc"] = []
+        globals()[f"{fold}_lr"] = []
+
+        globals()[f"{fold}_result"] = []
+        print(f"FOLD {fold}")
+        print("--------------------------------")
+
+        """model compile"""
+        model = EfficientNet.from_name(f"{config.model}")
+
+        """optimizer & loss"""
+
+        optimizer = RAdam(model.parameters(), lr=config.defalut_learning_rate)
+        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
+            optimizer,
+            mode="min",
+            factor=config.scheduler_factor,
+            patience=config.scheduler_patience,
+            threshold=0.0001,
+            cooldown=0,
+            min_lr=0,
+            verbose=1,
+        )
+        criterion = nn.BCELoss()
+        # criterion = nn.CrossEntropyLoss()
+        print("lr: ", optimizer.param_groups[0]["lr"])
+        state_dict = torch.load(
+            f"/home/hb/python/phospho/saved_model/0224/DeepPP_pretrain_1090_1708_bs1024_weight0/{config.pretrain_fold_num}fold_best_model.pth"
+        )
+        model.load_state_dict(state_dict["state_dict"])
+        model = model.to(device)
+        criterion.to(device)
+
+        best_model_weights = model.state_dict()
+        best_loss = 1000000.0
+
+        # Define data loaders for training and testing data in this fold
+        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)
+        test_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)
+
+        # Define data loaders for training and testing data in this fold
+        trainloader = torch.utils.data.DataLoader(
+            dataset, batch_size=config.batch_size, sampler=train_subsampler
+        )
+        validloader = torch.utils.data.DataLoader(
+            dataset, batch_size=config.batch_size, sampler=test_subsampler
+        )
+
+        early_stopping = EarlyStopping(patience=config.erls_patience, verbose=True)
+
+        for epoch in tqdm(range(config.n_epoch), position=0, leave=True):
+            print("-" * 60)
+            print("Epoch {}/{}".format(epoch + 1, config.n_epoch))
+
+            train_corrects = 0.0
+            train_loss = 0.0
+            train_precision, train_recall, train_f1 = 0.0, 0.0, 0.0
+            # rus = RandomUnderSampler(random_state=epoch)
+            # undersampling_idx, _ = rus.fit_resample(train_idx, y_train)
+            # undersampling_idx = undersampling_idx.flatten()
+
+            # X_urs = X_train[undersampling_idx]
+            # y_urs = y_train[undersampling_idx]
+            # print(y_urs.bincount())
+            # train_dataset = data_utils.TensorDataset(torch.tensor(X_urs), torch.tensor(y_urs))
+
+            # trainloader = torch.utils.data.DataLoader(
+            #                 train_dataset,
+            #                 batch_size=config.batch_size,)
+            # validloader = torch.utils.data.DataLoader(
+            #                 valid_dataset,
+            #                 batch_size=config.batch_size,)
+            for _, (inputs, labels) in enumerate(
+                tqdm(trainloader, position=1, leave=True)
+            ):
+                model.train(True)
+                # inputs, labels = data
+                inputs = Variable(
+                    inputs.to(device, dtype=torch.float), requires_grad=True
+                )
+                # print(labels.shape)
+                # print(labels)
+                # labels = labels.unsqueeze(1).to(device, dtype=torch.float)
+                # print(labels)
+                # print(labels.shape)
+                labels = Variable(labels.to(device))
+                # print(labels)
+
+                pred = model(inputs)  # forward
+                precision, recall, f1 = precision_recall(
+                    labels.float().view(-1, 1), pred
+                )  # outputs = net(inputs)
+                # loss = criterion(pred, labels)
+                loss = criterion(pred, labels.float().view(-1, 1)).to(device)
+                preds = (pred > 0.5).float()
+
+                """backward"""
+                optimizer.zero_grad()  # zero the parameter gradients
+                loss.backward()
+                optimizer.step()
+
+                """train record"""
+                train_loss += loss.item()
+                train_preds = (pred >= 0.5).float()
+                train_corrects += accuracy_score(labels.cpu(), train_preds.cpu())
+                train_precision += precision.detach()
+                train_recall += recall.detach()
+                train_f1 += f1.detach()
+
+            """epoch train record"""
+            epoch_train_loss = train_loss / len(trainloader)
+            epoch_train_precision = train_precision / len(trainloader)
+            epoch_train_recall = train_recall / len(trainloader)
+            epoch_train_f1 = train_f1 / len(trainloader)
+            epoch_train_acc = train_corrects / len(trainloader)
+            # # ---train 1 epoch ---
+
+            # ---valid 1 epoch
+            with torch.no_grad():
+                model.eval()
+
+                valid_corrects = 0.0
+                valid_loss = 0.0
+                valid_precision, valid_recall, valid_f1 = 0.0, 0.0, 0.0
+
+                for i, (inputs, labels) in enumerate(
+                    tqdm(validloader, position=1, leave=True)
+                ):
+                    # model.train(False)
+                    inputs = Variable(
+                        inputs.to(device, dtype=torch.float), requires_grad=True
+                    )
+                    labels = Variable(labels.to(device))
+
+                    pred = model(inputs)
+                    precision, recall, f1 = precision_recall(
+                        labels.float().view(-1, 1), pred
+                    )  # outputs = net(inputs)
+                    loss = criterion(pred, labels.float().view(-1, 1)).to(device)
+                    # loss = criterion(pred, labels).to(device)
+
+                    """valid record"""
+                    valid_loss += loss.item()
+                    valid_preds = (pred >= 0.5).float()
+                    valid_corrects += accuracy_score(labels.cpu(), valid_preds.cpu())
+                    valid_precision += precision.detach()
+                    valid_recall += recall.detach()
+                    valid_f1 += f1.detach()
+
+            """epoch valid record"""
+            epoch_valid_loss = valid_loss / len(validloader)
+            epoch_valid_precision = valid_precision / len(validloader)
+            epoch_valid_recall = valid_recall / len(validloader)
+            epoch_valid_f1 = valid_f1 / len(validloader)
+            epoch_valid_acc = valid_corrects / len(validloader)
+
+            globals()[f"{fold}_train_loss"].append(epoch_train_loss)
+            globals()[f"{fold}_train_precision"].append(epoch_train_precision)
+            globals()[f"{fold}_train_recall"].append(epoch_train_recall)
+            globals()[f"{fold}_train_f1"].append(epoch_train_f1)
+            globals()[f"{fold}_train_acc"].append(epoch_train_acc)
+
+            globals()[f"{fold}_valid_loss"].append(epoch_valid_loss)
+            globals()[f"{fold}_valid_precision"].append(epoch_valid_precision)
+            globals()[f"{fold}_valid_recall"].append(epoch_valid_recall)
+            globals()[f"{fold}_valid_f1"].append(epoch_valid_f1)
+            globals()[f"{fold}_valid_acc"].append(epoch_valid_acc)
+
+            if epoch_valid_loss < best_loss:
+                best_loss = epoch_valid_loss
+                best_model_weights = model.state_dict()
+            # valiid 1 epoch end
+            #    
+            checkpoint = {
+                "epoch": epoch,
+                "loss": epoch_valid_loss,
+                "model": model,
+                # 'state_dict': model.module.state_dict(),
+                "state_dict": model.state_dict(),
+                "optimizer": optimizer.state_dict(),
+            }
+            torch.save(checkpoint, f"{config.save_dir}/{fold}fold_latest_epoch.pth")
+
+            # Earlystopping & best  
+            savePath = "{}/{}fold_best_model.pth".format(wandb.config.save_dir, fold)
+            early_stopping(epoch_valid_loss, model, optimizer, savePath)
+            if early_stopping.early_stop:
+                print(
+                    f"Early stopping... fold:{fold} epoch:{epoch} loss:{epoch_valid_loss}"
+                )
+                break
+            # wandb.log({f'{config.data} : {fold}fold Validation loss': epoch_valid_loss, f'{fold}fold Learning_rate':optimizer.param_groups[0]['lr']})
+
+            wandb.log(
+                {
+                    f"{fold} fold train": {"loss": epoch_train_loss},
+                    f"{fold} fold val": {"loss": epoch_valid_loss},
+                    f"{fold} fold learning_rate": optimizer.param_groups[0]["lr"],
+                }
+            )
+            globals()[f"{fold}_lr"].append(optimizer.param_groups[0]["lr"])
+            scheduler.step(epoch_valid_loss)  # reduced  epoch backward
+            print("lr: ", optimizer.param_groups[0]["lr"])
+            print("-" * 60)
+            print()
+            # globals()[f'{fold}_result'].append(epoch_valid_loss)
+
+        torch.cuda.empty_cache()
+
+    plt.plot(globals()["0_valid_loss"], label="0fold")
+    plt.plot(globals()["1_valid_loss"], label="1fold")
+    plt.plot(globals()["2_valid_loss"], label="2fold")
+    plt.plot(globals()["3_valid_loss"], label="3fold")
+    plt.plot(globals()["4_valid_loss"], label="4fold")
+    plt.title("Validation loss")
+    plt.xlabel("epoch")
+    plt.ylabel("Validation loss")
+    plt.legend()
+    plt.show()
+    plt.savefig(config.save_dir + "/fig_saved.png")
+    # wandb.log({f'{config.data}': plt})
+    wandb.run.save()
+    wandb.finish()
+
+    print("Best val Loss: {:4f}".format(best_loss))
+    # load best model weights
+    model.load_state_dict(best_model_weights)
+    return model
+
+
+train_model_5cv()
diff --git a/utils.py b/utils.py
index be18dcb..cace508 100644
--- a/utils.py
+++ b/utils.py
@@ -586,10 +586,9 @@ def efficientnet(
     num_classes=1,
 ):
     """Creates a efficientnet model."""
-
     blocks_args = [
-        "r1_ckh3_ckw1_pkh0_pkw1_csh1_csw1_psh3_psw1_e1_i8_o16_se0.25",
-        # "r1_ckh3_ckw1_pkh0_pkw0_csh1_csw1_psh2_psw1_e1_i4_o8_se0.25",
+        "r1_ckh5_ckw1_pkh0_pkw1_csh1_csw1_psh3_psw1_e1_i8_o6_se0.25",
+        # "r2_ckh3_ckw3_pkh0_pkw0_csh2_csw2_psh2_psw1_e6_i6_o1_se0.25",
         # "r1_ckh3_ckw3_pkh2_pkw2_csh1_csw1_psh2_psw2_e1_i8_o16_se0.25",
         # 'r3_k3_s22_e6_i40_o80_se0.25',
         # 'r3_k5_s11_e6_i80_o112_se0.25',
diff --git a/utils2.py b/utils2.py
index 2dc7f46..e94d35c 100644
--- a/utils2.py
+++ b/utils2.py
@@ -588,14 +588,13 @@ def efficientnet(
     """Creates a efficientnet model."""
 
     blocks_args = [
-        "r1_ckh5_ckw5_pkh0_pkw1_csh2_csw1_psh3_psw1_e1_i8_o16_se0.25",
-        # "r1_ckh3_ckw1_pkh0_pkw0_csh1_csw1_psh2_psw1_e1_i4_o8_se0.25",
-        # "r1_ckh3_ckw3_pkh2_pkw2_csh1_csw1_psh2_psw2_e1_i8_o16_se0.25",
-        # 'r3_k3_s22_e6_i40_o80_se0.25',
-        # 'r3_k5_s11_e6_i80_o112_se0.25',
-        # 'r4_k5_s22_e6_i112_o192_se0.25',
-        # 'r1_k3_s11_e6_i192_o320_se0.25',
-        # 'r1_k3_s11_e6_i24_o48_se0.25',
+        "r1_ckh5_ckw1_pkh0_pkw1_csh1_csw1_psh3_psw1_e1_i8_o6_se0.25",
+        # "r2_ckh3_ckw3_pkh0_pkw1_csh2_csw1_psh2_psw1_e6_i6_o12_se0.25",
+        # "r2_ckh5_ckw5_pkh0_pkw2_csh2_csw2_psh2_psw2_e6_i12_o30_se0.25",
+        # "r3_ckh3_ckw3_pkh0_pkw2_csh2_csw1_psh2_psw2_e6_i30_o60_se0.25",
+        # "r3_ckh5_ckw5_pkh0_pkw2_csh1_csw1_psh2_psw2_e6_i60_o90_se0.25",      
+        # "r4_ckh5_ckw5_pkh0_pkw2_csh2_csw2_psh2_psw2_e6_i90_o120_se0.25",
+        # "r1_ckh3_ckw3_pkh0_pkw2_csh1_csw1_psh2_psw2_e6_i120_o200_se0.25",
     ]
     blocks_args = BlockDecoder.decode(blocks_args)
 
diff --git a/utils3.py b/utils3.py
index 420c7c8..586a7c2 100644
--- a/utils3.py
+++ b/utils3.py
@@ -588,7 +588,7 @@ def efficientnet(
     """Creates a efficientnet model."""
 
     blocks_args = [
-        "r1_ckh5_ckw3_pkh0_pkw1_csh2_csw1_psh3_psw1_e1_i8_o16_se0.25",
+        "r1_ckh5_ckw1_pkh0_pkw1_csh1_csw1_psh3_psw1_e1_i8_o6_se0.25",
         # "r1_ckh3_ckw1_pkh0_pkw0_csh1_csw1_psh2_psw1_e1_i4_o8_se0.25",
         # "r1_ckh3_ckw3_pkh2_pkw2_csh1_csw1_psh2_psw2_e1_i8_o16_se0.25",
         # 'r3_k3_s22_e6_i40_o80_se0.25',
diff --git a/utils4.py b/utils4.py
index 9804976..4918ad4 100644
--- a/utils4.py
+++ b/utils4.py
@@ -588,7 +588,7 @@ def efficientnet(
     """Creates a efficientnet model."""
 
     blocks_args = [
-        "r1_ckh5_ckw1_pkh0_pkw1_csh2_csw1_psh3_psw1_e1_i8_o16_se0.25",
+        "r1_ckh5_ckw1_pkh0_pkw1_csh1_csw1_psh3_psw1_e1_i8_o6_se0.25",
         # "r1_ckh3_ckw1_pkh0_pkw0_csh1_csw1_psh2_psw1_e1_i4_o8_se0.25",
         # "r1_ckh3_ckw3_pkh2_pkw2_csh1_csw1_psh2_psw2_e1_i8_o16_se0.25",
         # 'r3_k3_s22_e6_i40_o80_se0.25',
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index c3fa3a1..b33f904 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20230515_170843-jr2qjj7w/logs/debug-internal.log
\ No newline at end of file
+run-20230518_133138-dfg7dxgf/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index e29b7be..ab0b094 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20230515_170843-jr2qjj7w/logs/debug.log
\ No newline at end of file
+run-20230518_133138-dfg7dxgf/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 8d3442e..d88a0f1 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20230515_170843-jr2qjj7w
\ No newline at end of file
+run-20230518_133138-dfg7dxgf
\ No newline at end of file
